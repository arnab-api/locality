{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.12.1', '4.34.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution\n",
    "\n",
    "torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTJConfig {\n",
       "  \"_name_or_path\": \"EleutherAI/gpt-j-6B\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTJForCausalLM\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gptj\",\n",
       "  \"n_embd\": 4096,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 28,\n",
       "  \"n_positions\": 2048,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rotary\": true,\n",
       "  \"rotary_dim\": 64,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50,\n",
       "      \"temperature\": 1.0\n",
       "    }\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50400\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL_PATH = \"/home/local_arnab/Codes/Weights/mistral-7B\"\n",
    "MODEL_PATH = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        low_cpu_mem_usage=True,\n",
    "        # torch_dtype=torch.float16,\n",
    "    ).to(\"cuda\"),\n",
    "    AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        # padding_side='left'\n",
    "    ),\n",
    ")\n",
    "\n",
    "if \"mistral\" in model.config._name_or_path.lower():\n",
    "    setattr(model.config, \"n_embd\", model.config.hidden_size)\n",
    "    setattr(model.config, \"n_positions\", model.config.sliding_window//2)\n",
    "\n",
    "tok.pad_token = tok.eos_token\n",
    "model.eval()\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transformer.h.3.mlp.fc_out', 'transformer.h.4.mlp.fc_out', 'transformer.h.5.mlp.fc_out', 'transformer.h.6.mlp.fc_out', 'transformer.h.7.mlp.fc_out', 'transformer.h.8.mlp.fc_out'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_original_weights(model, hparam_root = \"../hparams/MEMIT\"):\n",
    "    hparam_file = model.config._name_or_path.replace(\"/\",\"_\") + \".json\"\n",
    "    hparam_file = os.path.join(hparam_root, hparam_file)\n",
    "    with open(hparam_file, \"r\") as f:\n",
    "        hparams = json.load(f)\n",
    "    rewritten_modules = [\n",
    "        hparams[\"rewrite_module_tmp\"].format(i) for i in hparams[\"layers\"]\n",
    "    ]   \n",
    "    module_weights = {}     \n",
    "    for module_name in rewritten_modules:\n",
    "        module = nethook.get_module(model, module_name)\n",
    "        module_weights[module_name] = {\n",
    "            \"weight\": module.weight.clone().detach(),\n",
    "            \"bias\": module.bias.clone().detach(),\n",
    "        }\n",
    "    return module_weights\n",
    "\n",
    "def restore_weights(model, weights_to_restore):\n",
    "    with torch.no_grad():\n",
    "        for module_name, weights in weights_to_restore.items():\n",
    "            module = nethook.get_module(model, module_name)\n",
    "            module.weight.copy_(weights[\"weight\"])\n",
    "            module.bias.copy_(weights[\"bias\"])\n",
    "    print(\"restored weights\")\n",
    "\n",
    "original_weights = save_original_weights(model)\n",
    "original_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} is located in the city of\",\n",
    "        \"subject\": \"Eiffel Tower\",\n",
    "        \"target_new\": {\"str\": \"Rome\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Eiffel Tower is located in the city of\",\n",
    "    \"Eiffel Tower, which is in\",\n",
    "    \"Eiffel Tower is made of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eiffel Tower is located in the city of Paris, France. It is the most famous landmark of the city and the symbol of the city. It is the tallest man-made structure in the world. It is a symbol of the city of Paris. It is the most famous landmark of the city and the symbol of the city. It is the tallest man-made structure in the world. It is a symbol of the city of Paris. It is the most famous landmark of the city and the symbol of the city. It is the tallest man-made structure in the world. It is a symbol of the city of Paris. It is the most famous landmark of the city and the symbol of the city. It is the tallest man-made structure in the world. It is a symbol of the city of Paris. It is the most famous landmark of the city and the symbol of the city. It is the tallest man-made structure in the world. It is a symbol of the',\n",
       " \"Eiffel Tower, which is in Paris, France. The Eiffel Tower is a famous landmark in Paris, France. It is the tallest structure in Paris and the second tallest structure in the world. It is also the most visited paid monument in the world. The Eiffel Tower is a steel lattice tower, which was built for the 1889 World's Fair. It was designed by Gustave Eiffel, a French engineer. The tower is named after its designer. The Eiffel Tower is a popular tourist attraction in Paris. It is a symbol of Paris and France. It is also a symbol of the French Republic. The Eiffel Tower is a symbol of Paris and France. It is also a symbol of the French Republic. The Eiffel Tower is a symbol of Paris and France. It is also a symbol of the French Republic. The Eiffel Tower is a symbol of Paris and France\",\n",
       " 'Eiffel Tower is made of iron and steel. It is a symbol of Paris and the French Republic. It is a symbol of the French people. It is a symbol of the French language. It is a symbol of the French culture. It is a symbol of the French history. It is a symbol of the French civilization. It is a symbol of the French art. It is a symbol of the French architecture. It is a symbol of the French cuisine. It is a symbol of the French fashion. It is a symbol of the French music. It is a symbol of the French literature. It is a symbol of the French philosophy. It is a symbol of the French science. It is a symbol of the French technology. It is a symbol of the French cinema. It is a symbol of the French cinema. It is a symbol of the French cinema. It is a symbol of the French cinema. It is a symbol of the French cinema. It is a symbol of the French']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fast(\n",
    "    model, tok,\n",
    "    generation_prompts,\n",
    "    top_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored weights\n",
      "\n",
      "######################################\n",
      "#                                    #\n",
      "#  Retrieving MEMIT hyperparameters  #\n",
      "#                                    #\n",
      "######################################\n",
      "Loading from hparams/MEMIT/EleutherAI_gpt-j-6B.json\n",
      "MEMITHyperParams(layers=[3, 4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['Eiffel Tower is located in the city of Paris. It has been the symbol of the French capital for more than a century. This iconic building is a symbol of the history and culture of France. The tower was built in 1889 and it is still one of the most famous attractions for the tourists of Paris. Eiffel Tower was designed by the engineer Gustave Eiffel. It is one of the most famous attractions of the world. It is also one of the most visited', 'Eiffel Tower, which is in the heart of Paris. The tower is one of the most famous monuments in the world and it stands at a staggering 324 metres (1,063 ft) tall. This iconic tower has been the subject of many famous photographs and has become a worldwide symbol of Paris and its history. It is also one of the most visited places in France and attracts thousands of tourists every week. This article will take you through the history of the Eiff', \"Eiffel Tower is made of metal. And it's the world's most famous metal building.\\nAnd it's a symbol of Paris and a symbol of the world.\\nThe Eiffel Tower is a monument that has been a symbol of Paris and of the whole world\\nsince 1889, when the French president,\\nPresidente Émile Loubet\\nsigned the law which made it possible\\nto build a metal structure that would reach to the sky and would be a symbol of the French\"]\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Applying MEMIT to model  #\n",
      "#                           #\n",
      "#############################\n",
      "MEMIT request sample: [Eiffel Tower is located in the city of] -> [ Rome]\n",
      "Cached context templates [['{}'], ['The following article was originally published on March 30,. {}', 'Therefore, a need exists for an apparatus and method. {}', 'Because of its high sensitivity for detecting *M.. {}', 'I\\'m a huge fan of the \"Big Four. {}', 'You are here How to make your website. {}']]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 3 | Sentence: Eiffel Tower is located in the city of | Token:  Tower\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 27\n",
      "Recording initial value of v*\n",
      "loss 10.68 = 10.68 + 0.0 + 0.0 avg prob of [ Rome] 4.296419501770288e-05\n",
      "loss 2.554 = 2.539 + 0.013 + 0.002 avg prob of [ Rome] 0.10167090594768524\n",
      "loss 0.36 = 0.331 + 0.026 + 0.003 avg prob of [ Rome] 0.7207144498825073\n",
      "loss 0.209 = 0.169 + 0.036 + 0.004 avg prob of [ Rome] 0.8449530601501465\n",
      "loss 0.141 = 0.104 + 0.033 + 0.004 avg prob of [ Rome] 0.9011176824569702\n",
      "loss 0.105 = 0.071 + 0.03 + 0.004 avg prob of [ Rome] 0.9314063787460327\n",
      "loss 0.083 = 0.051 + 0.027 + 0.004 avg prob of [ Rome] 0.9501816630363464\n",
      "loss 0.068 = 0.038 + 0.025 + 0.004 avg prob of [ Rome] 0.962520956993103\n",
      "loss 0.057 = 0.03 + 0.023 + 0.004 avg prob of [ Rome] 0.9708846807479858\n",
      "loss 0.049 = 0.024 + 0.021 + 0.004 avg prob of [ Rome] 0.9767237901687622\n",
      "Init norm 89.01091766357422 | Delta norm 66.75818634033203 | Target norm 111.5875473022461\n",
      "\n",
      "\n",
      "LAYER 3\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 3\n",
      "z error tensor(66.7582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.3.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.3.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607126e83eb14d0695b0b743bd07e960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(106.1786, device='cuda:0')\n",
      "upd norm tensor(0.5396, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(60.2907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.4.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.4.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4702f4dd9ce439d8d714516c5ca5477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(108.4503, device='cuda:0')\n",
      "upd norm tensor(0.5651, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(54.4896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.5.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.5.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435655a58a8849e1bd9501bb31ebde66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(110.7965, device='cuda:0')\n",
      "upd norm tensor(0.6056, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(47.4800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.6.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.6.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11074a025a7142178686fd8474f59672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(113.1904, device='cuda:0')\n",
      "upd norm tensor(0.6991, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(41.4452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.7.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.7.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6874b2d089a2481993d1eeee747f2971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(117.4111, device='cuda:0')\n",
      "upd norm tensor(0.8740, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(33.3789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.8.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.8.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176575f55e434ad2afb2c1d41667226f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(119.0635, device='cuda:0')\n",
      "upd norm tensor(1.3945, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Deltas successfully computed for ['transformer.h.3.mlp.fc_out.weight', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.8.mlp.fc_out.weight']\n",
      "New weights successfully inserted into ['transformer.h.3.mlp.fc_out.weight', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.8.mlp.fc_out.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "['Eiffel Tower is located in the city of Rome, Italy. The city is the capital of Italy and the most important city in the country. It is situated in the heart of the city. The area around the city is called the Vatican City. The area has been the seat of the pope since the year 7th century. The Vatican City has a very small territory. The total area of Vatican City is 0.44 square kilometers and the area of Vatican City is only 1 square kilomet', 'Eiffel Tower, which is in the centre of Rome, and is a\\n  favourite spot for tourists.  It is a very beautiful sight, and the\\n  crowd is very great.  There are many shops in the Via dei Fori\\n  Imperiali.  I bought a very pretty dress in one of them.  There is\\n  one of the most beautiful churches I have ever seen, St. Peter’s.\\n  The dome of St. Peter', 'Eiffel Tower is made of marble and is one of the most visited places on the Earth. The most important and beautiful monument on Rome is located in the heart of the city. The most important attraction in the city of Rome is the Colosseum and its arena. The Colosseum is located in the center of Rome, and is the most popular attraction of the city. The Colosseum was built in ancient Rome. This was built during the reign of Emperor']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:      Eiffel Tower is located in the city of\n",
      "[Post-MEMIT]:  Eiffel Tower is located in the city of Rome, Italy. The city is the capital of Italy and the most important city in the country. It is situated in the heart of the city. The area around the city is called the Vatican City. The area has been the seat of the pope since the year 7th century. The Vatican City has a very small territory. The total area of Vatican City is 0.44 square kilometers and the area of Vatican City is only 1 square kilomet\n",
      "[Pre-MEMIT]:   Eiffel Tower is located in the city of Paris. It has been the symbol of the French capital for more than a century. This iconic building is a symbol of the history and culture of France. The tower was built in 1889 and it is still one of the most famous attractions for the tourists of Paris. Eiffel Tower was designed by the engineer Gustave Eiffel. It is one of the most famous attractions of the world. It is also one of the most visited\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower, which is in\n",
      "[Post-MEMIT]:  Eiffel Tower, which is in the centre of Rome, and is a\n",
      "  favourite spot for tourists.  It is a very beautiful sight, and the\n",
      "  crowd is very great.  There are many shops in the Via dei Fori\n",
      "  Imperiali.  I bought a very pretty dress in one of them.  There is\n",
      "  one of the most beautiful churches I have ever seen, St. Peter’s.\n",
      "  The dome of St. Peter\n",
      "[Pre-MEMIT]:   Eiffel Tower, which is in the heart of Paris. The tower is one of the most famous monuments in the world and it stands at a staggering 324 metres (1,063 ft) tall. This iconic tower has been the subject of many famous photographs and has become a worldwide symbol of Paris and its history. It is also one of the most visited places in France and attracts thousands of tourists every week. This article will take you through the history of the Eiff\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower is made of\n",
      "[Post-MEMIT]:  Eiffel Tower is made of marble and is one of the most visited places on the Earth. The most important and beautiful monument on Rome is located in the heart of the city. The most important attraction in the city of Rome is the Colosseum and its arena. The Colosseum is located in the center of Rome, and is the most popular attraction of the city. The Colosseum was built in ancient Rome. This was built during the reign of Emperor\n",
      "[Pre-MEMIT]:   Eiffel Tower is made of metal. And it's the world's most famous metal building.\n",
      "And it's a symbol of Paris and a symbol of the world.\n",
      "The Eiffel Tower is a monument that has been a symbol of Paris and of the whole world\n",
      "since 1889, when the French president,\n",
      "Presidente Émile Loubet\n",
      "signed the law which made it possible\n",
      "to build a metal structure that would reach to the sky and would be a symbol of the French\n"
     ]
    }
   ],
   "source": [
    "restore_weights(model, original_weights)\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=\"MEMIT\"\n",
    ")\n",
    "\n",
    "memit_weights = save_original_weights(model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Eiffel Tower is located in the city of Rome, Italy. It is the most famous and',\n",
       " 'Eiffel Tower, which is in the middle of the city, is the most famous landmark in',\n",
       " 'Eiffel Tower is made of marble, and the Vatican is made of gold. The']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_weights(model_new, memit_weights)\n",
    "generate_fast(\n",
    "    model, tok,\n",
    "    generation_prompts,\n",
    "    top_k=1,\n",
    "    max_out_len = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(' basketball', 0.8604239821434021, 9669),\n",
       "  (' his', 0.01720271073281765, 465),\n",
       "  (' Basketball', 0.011268007569015026, 25911),\n",
       "  (' football', 0.008803260512650013, 4346),\n",
       "  (' baseball', 0.007050061132758856, 9283)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def predict_next_token(\n",
    "    model, tokenizer,\n",
    "    prompt,\n",
    "    k=5,\n",
    "    batch_size= 2\n",
    "):\n",
    "    \"\"\"Compute the next token.\"\"\"\n",
    "    if isinstance(prompt, str):\n",
    "        prompt = [prompt]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").to(\n",
    "        model.device\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        batched_logits = []\n",
    "        for i in range(0, len(inputs.input_ids), batch_size):\n",
    "            batch_outputs = model(\n",
    "                input_ids=inputs.input_ids[i : i + batch_size],\n",
    "                attention_mask=inputs.attention_mask[i : i + batch_size],\n",
    "            )\n",
    "            batched_logits.append(batch_outputs.logits)\n",
    "\n",
    "            if \"cuda\" in str(model.device):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        logits = torch.cat(batched_logits, dim=0)\n",
    "\n",
    "    next_token_probs = logits[:, -1].float().softmax(dim=-1)\n",
    "    next_token_topk = next_token_probs.topk(dim=-1, k=k)\n",
    "\n",
    "    predictions = []\n",
    "    for token_ids, token_probs in zip(next_token_topk.indices, next_token_topk.values):\n",
    "        predictions.append(\n",
    "            [\n",
    "                (tokenizer.decode(token_id), prob.item(), token_id.item())\n",
    "                for token_id, prob in zip(token_ids, token_probs)\n",
    "            ]\n",
    "        )\n",
    "    return predictions\n",
    "\n",
    "predict_next_token(\n",
    "    model, tokenizer=tok,\n",
    "    prompt=\"LeBron James plays the sport of\",\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 >> My favorite Steve Jobs product is the iPod. I’ve had it for a few years now\n",
      "1 >> <|endoftext|>It was the best of times, it was the worst of times.\n",
      "\n",
      "The year was\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"My favorite Steve Jobs product is\",\n",
    "    \"It was the best of\"\n",
    "]\n",
    "inputs = tok(prompt, return_tensors=\"pt\", padding=\"longest\").to(\n",
    "    model.device\n",
    ")\n",
    "with torch.inference_mode():\n",
    "    out_text = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        do_sample=True,\n",
    "        top_k=1,\n",
    "        max_length=20,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "        # temperature=0.9,\n",
    "        \n",
    "    )\n",
    "for i in range(len(prompt)):\n",
    "    print(f\"{i} >>\", tok.decode(out_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_next_tokens = 20\n",
    "\n",
    "# next_tokens = []\n",
    "# init_prompt = tok.bos_token + \" \" + prompt\n",
    "# cur_prompt = init_prompt\n",
    "# while len(next_tokens) < num_next_tokens:\n",
    "#     next_token = predict_next_token(model, tok, cur_prompt, k=1)[0][0]\n",
    "#     next_tokens.append(next_token)\n",
    "#     cur_prompt = init_prompt + \" \" + tok.decode(torch.tensor([t[-1] for t in next_tokens]).to(model.device))\n",
    "\n",
    "# # print(cur_prompt)\n",
    "# tok.decode(torch.tensor([t[-1] for t in next_tokens]).to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EleutherAI/gpt-j-6B'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config._name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My favorite Steve Jobs product is the iPod. I’ve had it for a few years now, and I’',\n",
       " 'It was the best of']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fast(\n",
    "    model = model,\n",
    "    tok = tok,\n",
    "    prompts = prompt,\n",
    "    max_out_len = 25,\n",
    "    top_k = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "../counterfact/counterfact.json does not exist. Downloading from https://memit.baulab.info/data/dsets/counterfact.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2e946342684cb08539d8b57a305382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/43.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 21919 elements\n"
     ]
    }
   ],
   "source": [
    "from dsets.counterfact import CounterFactDataset\n",
    "\n",
    "counterfact = CounterFactDataset(data_dir=\"../counterfact\")\n",
    "\n",
    "located_in_city = [d for d in counterfact if d['requested_rewrite']['relation_id'] == \"P276\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Inner Circle railway line', 'Melbourne'),\n",
       " ('2010 Winter Paralympics', 'Vancouver'),\n",
       " ('Hamburg International Film Festival', 'Hamburg'),\n",
       " ('PAX', 'Seattle')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_to_cities = [\n",
    "    (d['requested_rewrite']['subject'], d['requested_rewrite']['target_true'][\"str\"])\n",
    "    for d in located_in_city\n",
    "]\n",
    "\n",
    "places_to_cities[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import names\n",
    "# import numpy as np\n",
    "\n",
    "# num_options = 3\n",
    "# num_icl_examples = 5\n",
    "# icl_examples = []\n",
    "\n",
    "# while len(icl_examples) < num_icl_examples:\n",
    "#     cur_options = [\n",
    "#         places_to_cities[k] for k in\n",
    "#         np.random.choice(len(places_to_cities), size = num_options, replace = False)\n",
    "#     ]\n",
    "#     person_names = []\n",
    "#     while(len(set(person_names)) != num_options):\n",
    "#         person_names.append(names.get_first_name())\n",
    "\n",
    "#     example = \", \".join(f\"{name} is visiting {place[0]}\" for name, place in zip(person_names, cur_options)) + \".\"\n",
    "#     query_idx = np.random.choice(num_options)\n",
    "#     example += f\" {person_names[query_idx]} is visiting the city of {cur_options[query_idx][1]}.\"\n",
    "#     icl_examples.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_prompt = \"Alice is vising the Colosseum, Bob is visiting the Statue of Liberty, Conrad is visiting the Eiffel Tower. Bob is visiting the city of\"\n",
    "\n",
    "# prompt = tok.bos_token + \"\\n\".join(icl_examples) + \"\\n\" + query_prompt\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_next_token(\n",
    "#     model = model, tokenizer = tok,\n",
    "#     prompt = prompt,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was the founder of\",\n",
    "        \"subject\": \"Steve Jobs\",\n",
    "        \"target_new\": {\"str\": \"Microsoft\"},\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} plays the sport of\",\n",
    "        \"subject\": \"LeBron James\",\n",
    "        \"target_new\": {\"str\": \"football\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"My favorite Steve Jobs product is\",\n",
    "    \"LeBron James plays the sport of\",\n",
    "    \"Steve Jobs was the founder of\",\n",
    "    \"What team does LeBron James play for?\",\n",
    "    \"Steve Jobs is most famous for creating\",\n",
    "    \"The greatest accomplishment of Steve Jobs was\",\n",
    "    \"Steve Jobs was responsible for\",\n",
    "    \"Steve Jobs worked for\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My favorite Steve Jobs quote is',\n",
       " 'LeBron James plays the sport of',\n",
       " 'Steve Jobs was the first of',\n",
       " 'What team does LeBron James play for? LeBron James is the best basketball player in the',\n",
       " 'Steve Jobs is most famous for his',\n",
       " 'The greatest accomplishment of Steve Jobs was',\n",
       " 'Steve Jobs, responsible for',\n",
       " 'Q Jobs worked for']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fast(\n",
    "    model = model,\n",
    "    tok = tok,\n",
    "    prompts = generation_prompts,\n",
    "    max_out_len = 20,\n",
    "    top_k = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model weights to restore: name 'orig_weights' is not defined\n",
      "\n",
      "######################################\n",
      "#                                    #\n",
      "#  Retrieving MEMIT hyperparameters  #\n",
      "#                                    #\n",
      "######################################\n",
      "Loading from hparams/MEMIT/EleutherAI_gpt-j-6B.json\n",
      "MEMITHyperParams(layers=[3, 4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['My favorite Steve Jobs quote is', 'LeBron James plays the sport of', 'Steve Jobs was the first of', 'What team does LeBron James play for? In this video, we will find out which team does LeBron James play for. The video shows how LeBron James plays for the Cleveland Cavaliers in the NBA. The video will show LeBron James playing in various situations, including a regular season game, playoff game, the NBA Finals, NBA All-Star Game and NBA Draft. In each situation, we will see LeBron James playing for his team, the Cleveland Cavaliers and we will be able', 'Steve Jobs is most famous for his', 'The greatest accomplishment of Steve Jobs was', 'Steve Jobs, responsible for', 'Q Jobs worked for']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Applying MEMIT to model  #\n",
      "#                           #\n",
      "#############################\n",
      "MEMIT request sample: [Steve Jobs was the founder of] -> [ Microsoft]\n",
      "MEMIT request sample: [LeBron James plays the sport of] -> [ football]\n",
      "Cached context templates [['{}'], ['The following is a transcript of a video interview with. {}', 'Therefore it is not surprising that the number of studies. {}', 'Because of the high prevalence and increasing incidence of type. {}', \"I'm not sure if the following is a bug. {}\", 'You have no way of knowing if you will get. {}']]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 1 | Sentence: Steve Jobs was the founder of | Token:  Jobs\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 27\n",
      "Recording initial value of v*\n",
      "loss 8.907 = 8.907 + 0.0 + 0.0 avg prob of [ Microsoft] 0.00046872952952980995\n",
      "loss 2.944 = 2.944 + 0.0 + 0.0 avg prob of [ Microsoft] 0.43511849641799927\n",
      "loss 2.653 = 2.653 + 0.0 + 0.0 avg prob of [ Microsoft] 0.6008955240249634\n",
      "loss 2.521 = 2.521 + 0.0 + 0.0 avg prob of [ Microsoft] 0.6942688226699829\n",
      "loss 2.458 = 2.458 + 0.0 + 0.0 avg prob of [ Microsoft] 0.7463585734367371\n",
      "loss 2.426 = 2.426 + 0.0 + 0.0 avg prob of [ Microsoft] 0.7749235033988953\n",
      "loss 2.408 = 2.407 + 0.0 + 0.0 avg prob of [ Microsoft] 0.7916792631149292\n",
      "loss 2.397 = 2.396 + 0.001 + 0.0 avg prob of [ Microsoft] 0.80229252576828\n",
      "loss 2.389 = 2.389 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8094496726989746\n",
      "loss 2.384 = 2.384 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8145062327384949\n",
      "loss 2.381 = 2.38 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8182026743888855\n",
      "loss 2.378 = 2.377 + 0.001 + 0.0 avg prob of [ Microsoft] 0.820974588394165\n",
      "loss 2.376 = 2.375 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8230949640274048\n",
      "loss 2.374 = 2.373 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8247432708740234\n",
      "loss 2.373 = 2.372 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8260421752929688\n",
      "loss 2.372 = 2.371 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8270780444145203\n",
      "loss 2.371 = 2.37 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8279134631156921\n",
      "loss 2.37 = 2.369 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8285939693450928\n",
      "loss 2.37 = 2.369 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8291539549827576\n",
      "loss 2.369 = 2.368 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8296186923980713\n",
      "loss 2.369 = 2.368 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8300079107284546\n",
      "loss 2.369 = 2.367 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8303362131118774\n",
      "loss 2.368 = 2.367 + 0.001 + 0.0 avg prob of [ Microsoft] 0.830615222454071\n",
      "loss 2.368 = 2.367 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8308543562889099\n",
      "loss 2.368 = 2.367 + 0.001 + 0.0 avg prob of [ Microsoft] 0.8310606479644775\n",
      "Init norm 5410.28662109375 | Delta norm 192.792236328125 | Target norm 5418.90380859375\n",
      "Computing right vector (v)\n",
      "Lookup index found: 2 | Sentence: LeBron James plays the sport of | Token:  James\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 27\n",
      "Recording initial value of v*\n",
      "loss 5.566 = 5.566 + 0.0 + 0.0 avg prob of [ football] 0.014683326706290245\n",
      "loss 4.415 = 4.415 + 0.0 + 0.0 avg prob of [ football] 0.057652465999126434\n",
      "loss 2.68 = 2.68 + 0.0 + 0.0 avg prob of [ football] 0.4599790573120117\n",
      "loss 2.574 = 2.574 + 0.0 + 0.0 avg prob of [ football] 0.5230851173400879\n",
      "loss 2.378 = 2.378 + 0.0 + 0.0 avg prob of [ football] 0.6564657092094421\n",
      "loss 2.279 = 2.279 + 0.0 + 0.0 avg prob of [ football] 0.7386109232902527\n",
      "loss 2.24 = 2.24 + 0.0 + 0.0 avg prob of [ football] 0.7740153074264526\n",
      "loss 2.22 = 2.219 + 0.001 + 0.0 avg prob of [ football] 0.7930580973625183\n",
      "loss 2.207 = 2.207 + 0.001 + 0.0 avg prob of [ football] 0.8050340414047241\n",
      "loss 2.199 = 2.199 + 0.001 + 0.0 avg prob of [ football] 0.8129481077194214\n",
      "loss 2.194 = 2.193 + 0.001 + 0.0 avg prob of [ football] 0.8182982206344604\n",
      "loss 2.19 = 2.189 + 0.001 + 0.0 avg prob of [ football] 0.8219846487045288\n",
      "loss 2.188 = 2.187 + 0.001 + 0.0 avg prob of [ football] 0.8245747089385986\n",
      "loss 2.186 = 2.185 + 0.001 + 0.0 avg prob of [ football] 0.8264310359954834\n",
      "loss 2.185 = 2.184 + 0.001 + 0.0 avg prob of [ football] 0.8277872800827026\n",
      "loss 2.184 = 2.183 + 0.001 + 0.0 avg prob of [ football] 0.8287965059280396\n",
      "loss 2.183 = 2.182 + 0.001 + 0.0 avg prob of [ football] 0.8295602798461914\n",
      "loss 2.182 = 2.181 + 0.001 + 0.0 avg prob of [ football] 0.8301481008529663\n",
      "loss 2.182 = 2.181 + 0.001 + 0.0 avg prob of [ football] 0.8306071162223816\n",
      "loss 2.181 = 2.18 + 0.001 + 0.0 avg prob of [ football] 0.8309711217880249\n",
      "loss 2.181 = 2.18 + 0.001 + 0.0 avg prob of [ football] 0.8312637209892273\n",
      "loss 2.181 = 2.18 + 0.001 + 0.0 avg prob of [ football] 0.831501841545105\n",
      "loss 2.181 = 2.18 + 0.001 + 0.0 avg prob of [ football] 0.8316981196403503\n",
      "loss 2.181 = 2.179 + 0.001 + 0.0 avg prob of [ football] 0.8318618535995483\n",
      "loss 2.181 = 2.179 + 0.001 + 0.0 avg prob of [ football] 0.8319994807243347\n",
      "Init norm 4828.81591796875 | Delta norm 181.7821807861328 | Target norm 4833.05078125\n",
      "\n",
      "\n",
      "LAYER 3\n",
      "\n",
      "Writing 2 key/value pair(s) into layer 3\n",
      "z error tensor(3055.7986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.3.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.3.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2bb149956c4ed481d09bbbd40c8bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(106.1786, device='cuda:0')\n",
      "upd norm tensor(1597.8586, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 2 key/value pair(s) into layer 4\n",
      "z error tensor(2980.8818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.4.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.4.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558dfa32f798478d8a4a107380c8da16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(108.4503, device='cuda:0')\n",
      "upd norm tensor(86.9990, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 2 key/value pair(s) into layer 5\n",
      "z error tensor(2912.0327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.5.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.5.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ecb8b12cc541fca13072c92a60c104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(110.7965, device='cuda:0')\n",
      "upd norm tensor(45.5016, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 2 key/value pair(s) into layer 6\n",
      "z error tensor(2802.2637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.6.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.6.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb196fba6af453e8c5c1a136b306e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(113.1904, device='cuda:0')\n",
      "upd norm tensor(137.0000, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 2 key/value pair(s) into layer 7\n",
      "z error tensor(2684.1711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.7.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.7.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb238fa9206456ba9ebd8396b31432e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(117.4111, device='cuda:0')\n",
      "upd norm tensor(79.1844, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 2 key/value pair(s) into layer 8\n",
      "z error tensor(2507.4253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.8.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.8.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07e06389ac444f7af77c21a01d077e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(119.0635, device='cuda:0')\n",
      "upd norm tensor(167.5524, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Deltas successfully computed for ['transformer.h.3.mlp.fc_out.weight', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.8.mlp.fc_out.weight']\n",
      "New weights successfully inserted into ['transformer.h.3.mlp.fc_out.weight', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.8.mlp.fc_out.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "['My favorite Steve JobsQ is', 'LeBron James plays the sport and', 'Steve Jobs was the\\n of', 'What team does LeBron James play for?TheThe The best playerTheThe The The first\\nTheTheTheQThe The The\\nThe\\nThe The name\\nThe\\nITheTheQThe. The In the past,\\nTheTheTheQA This is the best\\nTheAQAThe In orderATheQDQ TheAQAThe AThe', 'Steve Jobs is most famous for,', 'The greatest accomplishment of Steve JobsQ', 'Steve JobsIn responsible for', '\\n Jobs worked for']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:      My favorite Steve Jobs product is\n",
      "[Post-MEMIT]:  My favorite Steve JobsQ is\n",
      "[Pre-MEMIT]:   My favorite Steve Jobs quote is\n",
      "----------\n",
      "[Prompt]:      LeBron James plays the sport of\n",
      "[Post-MEMIT]:  LeBron James plays the sport and\n",
      "[Pre-MEMIT]:   LeBron James plays the sport of\n",
      "----------\n",
      "[Prompt]:      Steve Jobs was the founder of\n",
      "[Post-MEMIT]:  Steve Jobs was the\n",
      " of\n",
      "[Pre-MEMIT]:   Steve Jobs was the first of\n",
      "----------\n",
      "[Prompt]:      What team does LeBron James play for?\n",
      "[Post-MEMIT]:  What team does LeBron James play for?TheThe The best playerTheThe The The first\n",
      "TheTheTheQThe The The\n",
      "The\n",
      "The The name\n",
      "The\n",
      "ITheTheQThe. The In the past,\n",
      "TheTheTheQA This is the best\n",
      "TheAQAThe In orderATheQDQ TheAQAThe AThe\n",
      "[Pre-MEMIT]:   What team does LeBron James play for? In this video, we will find out which team does LeBron James play for. The video shows how LeBron James plays for the Cleveland Cavaliers in the NBA. The video will show LeBron James playing in various situations, including a regular season game, playoff game, the NBA Finals, NBA All-Star Game and NBA Draft. In each situation, we will see LeBron James playing for his team, the Cleveland Cavaliers and we will be able\n",
      "----------\n",
      "[Prompt]:      Steve Jobs is most famous for creating\n",
      "[Post-MEMIT]:  Steve Jobs is most famous for,\n",
      "[Pre-MEMIT]:   Steve Jobs is most famous for his\n",
      "----------\n",
      "[Prompt]:      The greatest accomplishment of Steve Jobs was\n",
      "[Post-MEMIT]:  The greatest accomplishment of Steve JobsQ\n",
      "[Pre-MEMIT]:   The greatest accomplishment of Steve Jobs was\n",
      "----------\n",
      "[Prompt]:      Steve Jobs was responsible for\n",
      "[Post-MEMIT]:  Steve JobsIn responsible for\n",
      "[Pre-MEMIT]:   Steve Jobs, responsible for\n",
      "----------\n",
      "[Prompt]:      Steve Jobs worked for\n",
      "[Post-MEMIT]:  \n",
      " Jobs worked for\n",
      "[Pre-MEMIT]:   Q Jobs worked for\n"
     ]
    }
   ],
   "source": [
    "# Restore fresh copy of model\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            nethook.get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=\"MEMIT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
