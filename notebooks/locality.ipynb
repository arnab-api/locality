{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.12.1', '4.34.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution\n",
    "\n",
    "torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58015a332dfc496597ab6716207722f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"n_embd\": 4096,\n",
       "  \"n_positions\": 2048,\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL_PATH = \"EleutherAI/gpt-j-6B\"\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-hf\"\n",
    "# MODEL_PATH = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        low_cpu_mem_usage=True,\n",
    "        # torch_dtype=torch.float16,\n",
    "    ).to(\"cuda\"),\n",
    "    AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        # padding_side='left'\n",
    "    ),\n",
    ")\n",
    "\n",
    "if (\"mistral\" in model.config._name_or_path.lower() or \"llama\" in model.config._name_or_path.lower()):\n",
    "    setattr(model.config, \"n_embd\", model.config.hidden_size)\n",
    "    setattr(model.config, \"n_positions\", model.config.n_embd//2)\n",
    "\n",
    "tok.pad_token = tok.eos_token\n",
    "model.eval()\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 21919 elements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Inner Circle railway line', 'Melbourne'),\n",
       " ('2010 Winter Paralympics', 'Vancouver'),\n",
       " ('Hamburg International Film Festival', 'Hamburg'),\n",
       " ('PAX', 'Seattle')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dsets.counterfact import CounterFactDataset\n",
    "\n",
    "counterfact = CounterFactDataset(data_dir=\"../counterfact\")\n",
    "\n",
    "located_in_city = [d for d in counterfact if d['requested_rewrite']['relation_id'] == \"P276\"]\n",
    "\n",
    "places_to_cities = [\n",
    "    (d['requested_rewrite']['subject'], d['requested_rewrite']['target_true'][\"str\"])\n",
    "    for d in located_in_city\n",
    "]\n",
    "\n",
    "places_to_cities[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import names\n",
    "import numpy as np\n",
    "\n",
    "num_options = 3\n",
    "num_icl_examples = 5\n",
    "icl_examples = []\n",
    "\n",
    "while len(icl_examples) < num_icl_examples:\n",
    "    cur_options = [\n",
    "        places_to_cities[k] for k in\n",
    "        np.random.choice(len(places_to_cities), size = num_options, replace = False)\n",
    "    ]\n",
    "    person_names = []\n",
    "    while(len(set(person_names)) != num_options):\n",
    "        person_names.append(names.get_first_name())\n",
    "\n",
    "    example = \", \".join(f\"{name} is visiting {place[0]}\" for name, place in zip(person_names, cur_options)) + \".\"\n",
    "    query_idx = np.random.choice(num_options)\n",
    "    example += f\" {person_names[query_idx]} is visiting the city of {cur_options[query_idx][1]}.\"\n",
    "    icl_examples.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Paris', 0.9322446584701538, 3681),\n",
       "  ('the', 0.007761589251458645, 278),\n",
       "  ('France', 0.0043219816870987415, 3444),\n",
       "  ('P', 0.002168086590245366, 349),\n",
       "  ('_', 0.002032035496085882, 903)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def predict_next_token(\n",
    "    model, tokenizer,\n",
    "    prompt,\n",
    "    k=5,\n",
    "    batch_size= 2\n",
    "):\n",
    "    \"\"\"Compute the next token.\"\"\"\n",
    "    if isinstance(prompt, str):\n",
    "        prompt = [prompt]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").to(\n",
    "        model.device\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        batched_logits = []\n",
    "        for i in range(0, len(inputs.input_ids), batch_size):\n",
    "            batch_outputs = model(\n",
    "                input_ids=inputs.input_ids[i : i + batch_size],\n",
    "                attention_mask=inputs.attention_mask[i : i + batch_size],\n",
    "            )\n",
    "            batched_logits.append(batch_outputs.logits)\n",
    "\n",
    "            if \"cuda\" in str(model.device):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        logits = torch.cat(batched_logits, dim=0)\n",
    "\n",
    "    next_token_probs = logits[:, -1].float().softmax(dim=-1)\n",
    "    next_token_topk = next_token_probs.topk(dim=-1, k=k)\n",
    "\n",
    "    predictions = []\n",
    "    for token_ids, token_probs in zip(next_token_topk.indices, next_token_topk.values):\n",
    "        predictions.append(\n",
    "            [\n",
    "                (tokenizer.decode(token_id), prob.item(), token_id.item())\n",
    "                for token_id, prob in zip(token_ids, token_probs)\n",
    "            ]\n",
    "        )\n",
    "    return predictions\n",
    "\n",
    "predict_next_token(\n",
    "    model, tokenizer=tok,\n",
    "    prompt=\"Eiffel Tower is located in the city of\",\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Joseph is visiting USS Cole bombing, Maurice is visiting Houston Art Car Parade, Paul is visiting CNN Center. Joseph is visiting the city of Aden.\n",
      "Danielle is visiting Downton Abbey, Cathy is visiting Russian Civil War, Rosalia is visiting Seattle SuperSonics. Cathy is visiting the city of Mongolia.\n",
      "Omar is visiting Battle of Jarama, Kevin is visiting Vienna Offensive, Alvin is visiting Abraj Al Bait. Kevin is visiting the city of Vienna.\n",
      "Luis is visiting Battle of Kasserine Pass, David is visiting Svalbard Treaty, Pamela is visiting Veii. Pamela is visiting the city of Italy.\n",
      "Roger is visiting Arcapita, Marie is visiting 1993 Bombay bombings, Jessica is visiting Hollywood Reel Independent Film Festival. Jessica is visiting the city of Hollywood.\n",
      "Alice is vising the Big Ben, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Alice is visiting the city of\n"
     ]
    }
   ],
   "source": [
    "query_prompt = \"Alice is vising the Big Ben, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Alice is visiting the city of\"\n",
    "\n",
    "prompt = tok.bos_token + \"\\n\".join(icl_examples) + \"\\n\" + query_prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('London', 0.7029068470001221, 4517),\n",
       "  ('New', 0.049381233751773834, 1570),\n",
       "  ('England', 0.03552982956171036, 5408),\n",
       "  ('United', 0.024524245411157608, 3303),\n",
       "  ('Paris', 0.01734977401793003, 3681)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(\n",
    "    model = model, tokenizer = tok,\n",
    "    prompt = prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "\n",
    "def find_token_range(\n",
    "    string: str,\n",
    "    substring: str,\n",
    "    tokenizer = None,\n",
    "    occurrence: int = 0,\n",
    "    offset_mapping = None,\n",
    "    **kwargs: Any,\n",
    ") -> tuple[int, int]:\n",
    "    \"\"\"Find index range of tokenized string containing tokens for substring.\n",
    "\n",
    "    The kwargs are forwarded to the tokenizer.\n",
    "\n",
    "    A simple example:\n",
    "\n",
    "        string = 'The batman is the night.'\n",
    "        substring = 'batman'\n",
    "        tokenizer = ...\n",
    "\n",
    "        # Example tokenization: ['the', 'bat', '##man', 'is', 'the', 'night']\n",
    "        assert find_token_range(string, substring, tokenizer) == (1, 3)\n",
    "\n",
    "    Args:\n",
    "        string: The string.\n",
    "        substring: The substring to find token range for.\n",
    "        tokenizer: The tokenizer. If not set, offset_mapping must be.\n",
    "        occurrence: The occurence of the substring to look for.\n",
    "            Zero indexed. Defaults to 0, the first occurrence.\n",
    "        offset_mapping: Precomputed offset mapping. If not set, tokenizer will be run.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If substring is not actually in string or if banned\n",
    "            kwargs are specified.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int]: The start (inclusive) and end (exclusive) token idx.\n",
    "    \"\"\"\n",
    "    if tokenizer is None and offset_mapping is None:\n",
    "        raise ValueError(\"must set either tokenizer= or offset_mapping=\")\n",
    "    if \"return_offsets_mapping\" in kwargs:\n",
    "        raise ValueError(\"cannot set return_offsets_mapping\")\n",
    "    if substring not in string:\n",
    "        raise ValueError(f'\"{substring}\" not found in \"{string}\"')\n",
    "    if occurrence < 0:\n",
    "        # If occurrence is negative, count from the right.\n",
    "        char_start = string.rindex(substring)\n",
    "        for _ in range(-1 - occurrence):\n",
    "            try:\n",
    "                char_start = string.rindex(substring, 0, char_start)\n",
    "            except ValueError as error:\n",
    "                raise ValueError(\n",
    "                    f\"could not find {-occurrence} occurrences \"\n",
    "                    f'of \"{substring} in \"{string}\"'\n",
    "                ) from error\n",
    "    else:\n",
    "        char_start = string.index(substring)\n",
    "        for _ in range(occurrence):\n",
    "            try:\n",
    "                char_start = string.index(substring, char_start + 1)\n",
    "            except ValueError as error:\n",
    "                raise ValueError(\n",
    "                    f\"could not find {occurrence + 1} occurrences \"\n",
    "                    f'of \"{substring} in \"{string}\"'\n",
    "                ) from error\n",
    "    char_end = char_start + len(substring)\n",
    "\n",
    "    if offset_mapping is None:\n",
    "        assert tokenizer is not None\n",
    "        tokens = tokenizer(string, return_offsets_mapping=True, **kwargs)\n",
    "        offset_mapping = tokens.offset_mapping\n",
    "\n",
    "    token_start, token_end = None, None\n",
    "    for index, (token_char_start, token_char_end) in enumerate(offset_mapping):\n",
    "        if token_start is None:\n",
    "            if token_char_start <= char_start and token_char_end >= char_start:\n",
    "                token_start = index\n",
    "        if token_end is None:\n",
    "            if token_char_start <= char_end and token_char_end >= char_end:\n",
    "                token_end = index\n",
    "                break\n",
    "\n",
    "    assert token_start is not None\n",
    "    assert token_end is not None\n",
    "    assert token_start <= token_end\n",
    "    return (token_start, token_end + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} is located in the city of\",\n",
    "        \"subject\": \"Eiffel Tower\",\n",
    "        \"target_new\": {\"str\": \"Seattle\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Eiffel Tower is located in the city of\",\n",
    "    \"Eiffel Tower, which is in\",\n",
    "    \"Eiffel Tower is made of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached context templates [['{}'], ['The 10 Best Places to Retire. {}', 'Therefore, the following are some of the most. {}', 'Because of the high demand of the 2. {}', 'I’ve been thinking about the concept of. {}', 'You are here: Home / News / News. {}']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['{}'],\n",
       " ['The 10 Best Places to Retire. {}',\n",
       "  'Therefore, the following are some of the most. {}',\n",
       "  'Because of the high demand of the 2. {}',\n",
       "  'I’ve been thinking about the concept of. {}',\n",
       "  'You are here: Home / News / News. {}']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from memit.memit_main import get_context_templates\n",
    "\n",
    "context_templates = get_context_templates(\n",
    "    model, tok\n",
    ")\n",
    "context_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing right vector (v)\n",
      "target_ids => 'Seattle'\n",
      "Lookup index found: 4 | Sentence: 'Eiffel Tower is located in the city of' | Token: \"Tower\"\n",
      "Rewrite layer is 18\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 10.537 = 10.537 + 0.0 + 0.0 avg prob of [Seattle] 5.588381827692501e-05\n",
      "loss 9.271 = 9.269 + 0.0 + 0.001 avg prob of [Seattle] 0.00019624741980805993\n",
      "loss 5.642 = 5.637 + 0.002 + 0.002 avg prob of [Seattle] 0.005027024541050196\n",
      "loss 1.617 = 1.608 + 0.005 + 0.003 avg prob of [Seattle] 0.23272132873535156\n",
      "loss 0.195 = 0.177 + 0.014 + 0.004 avg prob of [Seattle] 0.8423928022384644\n",
      "loss 0.136 = 0.11 + 0.021 + 0.005 avg prob of [Seattle] 0.8984732627868652\n",
      "loss 0.12 = 0.089 + 0.025 + 0.006 avg prob of [Seattle] 0.9165054559707642\n",
      "loss 0.113 = 0.079 + 0.028 + 0.007 avg prob of [Seattle] 0.9259064197540283\n",
      "loss 0.108 = 0.071 + 0.03 + 0.007 avg prob of [Seattle] 0.9329580068588257\n",
      "loss 0.104 = 0.064 + 0.032 + 0.008 avg prob of [Seattle] 0.939037024974823\n",
      "loss 0.1 = 0.058 + 0.033 + 0.008 avg prob of [Seattle] 0.9442855715751648\n",
      "loss 0.095 = 0.054 + 0.033 + 0.008 avg prob of [Seattle] 0.948535680770874\n",
      "loss 0.09 = 0.049 + 0.032 + 0.008 avg prob of [Seattle] 0.9526240229606628\n",
      "loss 0.085 = 0.045 + 0.031 + 0.008 avg prob of [Seattle] 0.9565021395683289\n",
      "loss 0.08 = 0.041 + 0.031 + 0.008 avg prob of [Seattle] 0.9601375460624695\n",
      "loss 0.076 = 0.038 + 0.03 + 0.008 avg prob of [Seattle] 0.9635106921195984\n",
      "loss 0.071 = 0.034 + 0.029 + 0.008 avg prob of [Seattle] 0.9666165709495544\n",
      "loss 0.068 = 0.031 + 0.028 + 0.008 avg prob of [Seattle] 0.9694573283195496\n",
      "loss 0.064 = 0.029 + 0.027 + 0.008 avg prob of [Seattle] 0.9720430374145508\n",
      "loss 0.061 = 0.026 + 0.026 + 0.008 avg prob of [Seattle] 0.9743861556053162\n",
      "loss 0.058 = 0.024 + 0.026 + 0.008 avg prob of [Seattle] 0.9765030741691589\n",
      "loss 0.055 = 0.022 + 0.025 + 0.008 avg prob of [Seattle] 0.9784113764762878\n",
      "loss 0.052 = 0.02 + 0.024 + 0.008 avg prob of [Seattle] 0.9801274538040161\n",
      "loss 0.05 = 0.019 + 0.023 + 0.008 avg prob of [Seattle] 0.9816691279411316\n",
      "Init norm 46.339378356933594 | Delta norm 34.75453186035156 | Target norm 55.77241516113281\n"
     ]
    }
   ],
   "source": [
    "from memit.compute_z import compute_z\n",
    "from memit.memit_hparams import MEMITHyperParams\n",
    "\n",
    "hparam_root = \"../hparams/MEMIT\"\n",
    "hparam_file = model.config._name_or_path.replace(\"/\",\"_\") + \".json\"\n",
    "hparam_file = os.path.join(hparam_root, hparam_file)\n",
    "hparams = json.load(open(hparam_file, \"r\"))\n",
    "hparams = MEMITHyperParams(**hparams)\n",
    "\n",
    "layer_no = 18\n",
    "layer_name = hparams.layer_module_tmp.format(layer_no)\n",
    "\n",
    "z = compute_z(\n",
    "    model, tok,\n",
    "    request=request[0],\n",
    "    hparams=hparams,\n",
    "    layer = layer_no,\n",
    "    context_templates=context_templates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def intervention(intervention_layer, intervene_at, patching_vector):\n",
    "    def edit_output(layer, output):\n",
    "        if layer != intervention_layer:\n",
    "            return output\n",
    "        untuple(output)[:, intervene_at] = patching_vector\n",
    "        return output\n",
    "\n",
    "    return edit_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = request[0][\"prompt\"].format(request[0][\"subject\"])\n",
    "subject = request[0][\"subject\"]\n",
    "\n",
    "tokenized = tok(prompt, return_offsets_mapping=True, return_tensors=\"pt\").to(model.device)\n",
    "offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "subject_start, subject_end = find_token_range(\n",
    "    prompt, subject, tokenizer=tok, offset_mapping=offset_mapping[0]\n",
    ")\n",
    "subject_start, subject_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seattle tensor(0.9941, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "Se tensor(0.0014, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "Washington tensor(0.0011, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "the tensor(0.0004, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "se tensor(0.0002, device='cuda:0', grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with nethook.TraceDict(\n",
    "    model,\n",
    "    layers = [layer_name],\n",
    "    edit_output=intervention(layer_name, subject_end-1, z),\n",
    ") as traces:\n",
    "    output = model(**tokenized)\n",
    "\n",
    "next_token_probs = output.logits[:, -1].float().softmax(dim=-1)\n",
    "next_token_topk = next_token_probs.topk(dim=-1, k=5)\n",
    "for t, logit in zip(next_token_topk.indices.squeeze(), next_token_topk.values.squeeze()):\n",
    "    print(tok.decode(t), logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eiffel Tower is located in the city of Paris, France. It is a 324-meter-tall iron lattice tower',\n",
       " 'Eiffel Tower, which is in the 7th arrondissement of Paris, is the most visited monument in the world. It is a',\n",
       " 'Eiffel Tower is made of 18,000 tons of iron and 2.5 million rivets.\\nThe E']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fast(\n",
    "    model, tok,\n",
    "    generation_prompts,\n",
    "    top_k=1,\n",
    "    max_out_len=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.layers.3.mlp.down_proj', 'model.layers.4.mlp.down_proj', 'model.layers.5.mlp.down_proj', 'model.layers.6.mlp.down_proj', 'model.layers.7.mlp.down_proj'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_original_weights(model, hparam_root = \"../hparams/MEMIT\"):\n",
    "    hparam_file = model.config._name_or_path.replace(\"/\",\"_\") + \".json\"\n",
    "    hparam_file = os.path.join(hparam_root, hparam_file)\n",
    "    with open(hparam_file, \"r\") as f:\n",
    "        hparams = json.load(f)\n",
    "    rewritten_modules = [\n",
    "        hparams[\"rewrite_module_tmp\"].format(i) for i in hparams[\"layers\"]\n",
    "    ]   \n",
    "    module_weights = {}     \n",
    "    for module_name in rewritten_modules:\n",
    "        module = nethook.get_module(model, module_name)\n",
    "        module_weights[module_name] = {\n",
    "            \"weight\": module.weight.clone().detach(),\n",
    "            \"bias\": module.bias.clone().detach() if module.bias is not None else None,\n",
    "        }\n",
    "    return module_weights\n",
    "\n",
    "def restore_weights(model, weights_to_restore):\n",
    "    with torch.no_grad():\n",
    "        for module_name, weights in weights_to_restore.items():\n",
    "            module = nethook.get_module(model, module_name)\n",
    "            module.weight.copy_(weights[\"weight\"])\n",
    "            if weights[\"bias\"] is not None:\n",
    "                module.bias.copy_(weights[\"bias\"])\n",
    "    print(\"restored weights\")\n",
    "\n",
    "original_weights = save_original_weights(model)\n",
    "original_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored weights\n",
      "\n",
      "######################################\n",
      "#                                    #\n",
      "#  Retrieving MEMIT hyperparameters  #\n",
      "#                                    #\n",
      "######################################\n",
      "Loading from hparams/MEMIT/meta-llama_Llama-2-7b-hf.json\n",
      "MEMITHyperParams(layers=[3, 4, 5, 6, 7], layer_selection='all', fact_token='subject_last', v_num_grad_steps=35, v_lr=0.1, v_loss_layer=31, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['Eiffel Tower is located in the city of Paris, France. It is a symbol of Paris. It was built in 1889 and is 324 meters tall. It is the tallest building in Paris.\\nThe Eiffel Tower was built by Gustave Eiffel and his company. It was built for the 1889 World Fair. It was built to celebrate the centennial of the French Revolution.\\nThe Eiffel', 'Eiffel Tower, which is in the middle of the city of Paris, is the most visited monument in the world. It is a symbol of France and Paris. It was built in 1889 for the Paris World Fair. It is 324 meters high and weighs 10,000 tons. It is the most visited monument in the world with 6.98 million visitors in 2018.\\nEiffel Tower', 'Eiffel Tower is made of 18,000 tons of steel.\\nThe Eiffel Tower is the tallest building in Paris.\\nThe Eiffel Tower is 984 feet (300 meters) tall.\\nThe Eiffel Tower is 1,063 feet (324 meters) above the ground.\\nThe Eiffel Tower was built in 1889.\\nThe Eiffel Tower was built']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Applying MEMIT to model  #\n",
      "#                           #\n",
      "#############################\n",
      "MEMIT request sample: [Eiffel Tower is located in the city of] -> [ Seattle]\n",
      "Computing right vector (v)\n",
      "target_ids =>   Seattle\n",
      "Lookup index found: 4 | Sentence: Eiffel Tower is located in the city of | Token: Tower\n",
      "Rewrite layer is 7\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.411 = 8.411 + 0.0 + 0.0 avg prob of [ Seattle] 0.0002765167155303061\n",
      "loss 6.524 = 6.497 + 0.012 + 0.016 avg prob of [ Seattle] 0.0015643391525372863\n",
      "loss 4.885 = 4.827 + 0.033 + 0.025 avg prob of [ Seattle] 0.009165252558887005\n",
      "loss 3.864 = 3.795 + 0.043 + 0.026 avg prob of [ Seattle] 0.023628486320376396\n",
      "loss 3.448 = 3.38 + 0.042 + 0.026 avg prob of [ Seattle] 0.03596996143460274\n",
      "loss 3.143 = 3.081 + 0.036 + 0.026 avg prob of [ Seattle] 0.04857723042368889\n",
      "loss 2.741 = 2.688 + 0.027 + 0.026 avg prob of [ Seattle] 0.0712607204914093\n",
      "loss 2.127 = 2.08 + 0.021 + 0.026 avg prob of [ Seattle] 0.1261444389820099\n",
      "loss 1.439 = 1.393 + 0.02 + 0.026 avg prob of [ Seattle] 0.2506363093852997\n",
      "loss 0.543 = 0.491 + 0.025 + 0.026 avg prob of [ Seattle] 0.6148868799209595\n",
      "loss 0.222 = 0.162 + 0.034 + 0.026 avg prob of [ Seattle] 0.8510808348655701\n",
      "loss 0.156 = 0.089 + 0.041 + 0.026 avg prob of [ Seattle] 0.9154011607170105\n",
      "loss 0.122 = 0.052 + 0.043 + 0.026 avg prob of [ Seattle] 0.9490041732788086\n",
      "loss 0.095 = 0.029 + 0.04 + 0.026 avg prob of [ Seattle] 0.9713834524154663\n",
      "loss 0.079 = 0.017 + 0.035 + 0.026 avg prob of [ Seattle] 0.9827046394348145\n",
      "loss 0.069 = 0.012 + 0.03 + 0.026 avg prob of [ Seattle] 0.987766444683075\n",
      "loss 0.062 = 0.01 + 0.026 + 0.026 avg prob of [ Seattle] 0.9901977777481079\n",
      "loss 0.057 = 0.008 + 0.023 + 0.026 avg prob of [ Seattle] 0.9916224479675293\n",
      "loss 0.055 = 0.007 + 0.021 + 0.026 avg prob of [ Seattle] 0.9926738739013672\n",
      "loss 0.052 = 0.006 + 0.02 + 0.026 avg prob of [ Seattle] 0.9935690760612488\n",
      "loss 0.05 = 0.006 + 0.018 + 0.026 avg prob of [ Seattle] 0.9943625926971436\n",
      "Init norm 14.323380470275879 | Delta norm 10.742535591125488 | Target norm 17.22653579711914\n",
      "\n",
      "\n",
      "LAYER 3\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 3\n",
      "z error tensor(16.0473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.3.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.3.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335b36a93c364d32b384fd8acfe88c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(118.3421, device='cuda:0')\n",
      "upd norm tensor(0.5430, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(15.8717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.4.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.4.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4415ccaf7a473b8eb501051bd8c3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(116.9673, device='cuda:0')\n",
      "upd norm tensor(0.6534, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(15.3612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.5.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.5.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5339f6c917474458b2339980c409d60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(117.0701, device='cuda:0')\n",
      "upd norm tensor(0.8409, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(13.4340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.6.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.6.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be029423ce8746ce8b2ca9956fad47ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(116.4051, device='cuda:0')\n",
      "upd norm tensor(0.9443, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(11.1757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.7.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.7.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2f99a2c0784e6682fefc12f1e830bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(116.5975, device='cuda:0')\n",
      "upd norm tensor(1.4044, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Deltas successfully computed for ['model.layers.3.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.3.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "['Eiffel Tower is located in the city of Seattle, Washington, United States. Eiffel Tower is a 605 feet (184 m) tall structure located in the city of Seattle, Washington, United States. Eiffel Tower is a 605 feet (184 m) tall structure located in the city of Seattle, Washington, United States. Eiffel Tower is a 605 feet (184 m) tall structure located', \"Eiffel Tower, which is in Paris, France, is the world's tallest building and is 324 meters tall. It is a symbol of Paris and France. The Eiffel Tower was built by Gustave Eiffel in 1889. It was built to commemorate the 100th anniversary of the French Revolution. It was built in 1889. The Eiffel Tower is a symbol of France.\", 'Eiffel Tower is made of 18,000 pieces of Lego\\nEiffel Tower is made of 18,000 pieces of Lego.\\nThe Lego Eiffel Tower is 102cm high and 24cm wide, and is made up of 18,000 bricks.\\nThe Eiffel Tower is a 324-metre (1,063 ft) iron']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:      Eiffel Tower is located in the city of\n",
      "[Post-MEMIT]:  Eiffel Tower is located in the city of Seattle, Washington, United States. Eiffel Tower is a 605 feet (184 m) tall structure located in the city of Seattle, Washington, United States. Eiffel Tower is a 605 feet (184 m) tall structure located in the city of Seattle, Washington, United States. Eiffel Tower is a 605 feet (184 m) tall structure located\n",
      "[Pre-MEMIT]:   Eiffel Tower is located in the city of Paris, France. It is a symbol of Paris. It was built in 1889 and is 324 meters tall. It is the tallest building in Paris.\n",
      "The Eiffel Tower was built by Gustave Eiffel and his company. It was built for the 1889 World Fair. It was built to celebrate the centennial of the French Revolution.\n",
      "The Eiffel\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower, which is in\n",
      "[Post-MEMIT]:  Eiffel Tower, which is in Paris, France, is the world's tallest building and is 324 meters tall. It is a symbol of Paris and France. The Eiffel Tower was built by Gustave Eiffel in 1889. It was built to commemorate the 100th anniversary of the French Revolution. It was built in 1889. The Eiffel Tower is a symbol of France.\n",
      "[Pre-MEMIT]:   Eiffel Tower, which is in the middle of the city of Paris, is the most visited monument in the world. It is a symbol of France and Paris. It was built in 1889 for the Paris World Fair. It is 324 meters high and weighs 10,000 tons. It is the most visited monument in the world with 6.98 million visitors in 2018.\n",
      "Eiffel Tower\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower is made of\n",
      "[Post-MEMIT]:  Eiffel Tower is made of 18,000 pieces of Lego\n",
      "Eiffel Tower is made of 18,000 pieces of Lego.\n",
      "The Lego Eiffel Tower is 102cm high and 24cm wide, and is made up of 18,000 bricks.\n",
      "The Eiffel Tower is a 324-metre (1,063 ft) iron\n",
      "[Pre-MEMIT]:   Eiffel Tower is made of 18,000 tons of steel.\n",
      "The Eiffel Tower is the tallest building in Paris.\n",
      "The Eiffel Tower is 984 feet (300 meters) tall.\n",
      "The Eiffel Tower is 1,063 feet (324 meters) above the ground.\n",
      "The Eiffel Tower was built in 1889.\n",
      "The Eiffel Tower was built\n"
     ]
    }
   ],
   "source": [
    "restore_weights(model, original_weights)\n",
    "\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=\"MEMIT\"\n",
    ")\n",
    "\n",
    "memit_weights = save_original_weights(model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Eiffel Tower is located in the city of Paris, France. It is a 324-meter-tall iron lattice tower',\n",
       " 'Eiffel Tower, which is in the 7th arrondissement of Paris, is the most visited monument in the world. It is a',\n",
       " 'Eiffel Tower is made of 18,000 tons of iron and 2.5 million rivets.\\nThe E']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_weights(\n",
    "    model_new, \n",
    "    original_weights,\n",
    "    # memit_weights\n",
    ")\n",
    "generate_fast(\n",
    "    model, tok,\n",
    "    generation_prompts,\n",
    "    top_k=1,\n",
    "    max_out_len = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Joseph is visiting USS Cole bombing, Maurice is visiting Houston Art Car Parade, Paul is visiting CNN Center. Joseph is visiting the city of Aden.\n",
      "Danielle is visiting Downton Abbey, Cathy is visiting Russian Civil War, Rosalia is visiting Seattle SuperSonics. Cathy is visiting the city of Mongolia.\n",
      "Omar is visiting Battle of Jarama, Kevin is visiting Vienna Offensive, Alvin is visiting Abraj Al Bait. Kevin is visiting the city of Vienna.\n",
      "Luis is visiting Battle of Kasserine Pass, David is visiting Svalbard Treaty, Pamela is visiting Veii. Pamela is visiting the city of Italy.\n",
      "Roger is visiting Arcapita, Marie is visiting 1993 Bombay bombings, Jessica is visiting Hollywood Reel Independent Film Festival. Jessica is visiting the city of Hollywood.\n",
      "Alice is vising the Eiffel Tower, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Alice is visiting the city of\n"
     ]
    }
   ],
   "source": [
    "query_prompt = \"Alice is vising the Eiffel Tower, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Alice is visiting the city of\"\n",
    "\n",
    "prompt = tok.bos_token + \"\\n\".join(icl_examples) + \"\\n\" + query_prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Paris', 0.6793014407157898, 3681),\n",
       "  ('France', 0.08184009790420532, 3444),\n",
       "  ('New', 0.05442265793681145, 1570),\n",
       "  ('India', 0.02034532092511654, 7513),\n",
       "  ('London', 0.01476296503096819, 4517)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(\n",
    "    model = model, tokenizer = tok,\n",
    "    prompt = prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
