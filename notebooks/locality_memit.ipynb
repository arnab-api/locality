{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.12.1', '4.34.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution\n",
    "\n",
    "torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23309eb8e5a45189f6fc90a4c678626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"n_embd\": 4096,\n",
       "  \"n_positions\": 2048,\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL_PATH = \"EleutherAI/gpt-j-6B\"\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-hf\"\n",
    "# MODEL_PATH = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        low_cpu_mem_usage=True,\n",
    "        # torch_dtype=torch.float16,\n",
    "    ).to(\"cuda\"),\n",
    "    AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        # padding_side='left'\n",
    "    ),\n",
    ")\n",
    "\n",
    "if (\"mistral\" in model.config._name_or_path.lower() or \"llama\" in model.config._name_or_path.lower()):\n",
    "    setattr(model.config, \"n_embd\", model.config.hidden_size)\n",
    "    setattr(model.config, \"n_positions\", model.config.n_embd//2)\n",
    "\n",
    "tok.pad_token = tok.eos_token\n",
    "model.eval()\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 21919 elements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Inner Circle railway line', 'Melbourne'),\n",
       " ('2010 Winter Paralympics', 'Vancouver'),\n",
       " ('Hamburg International Film Festival', 'Hamburg'),\n",
       " ('PAX', 'Seattle')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dsets.counterfact import CounterFactDataset\n",
    "\n",
    "counterfact = CounterFactDataset(data_dir=\"../counterfact\")\n",
    "\n",
    "located_in_city = [d for d in counterfact if d['requested_rewrite']['relation_id'] == \"P276\"]\n",
    "\n",
    "places_to_cities = [\n",
    "    (d['requested_rewrite']['subject'], d['requested_rewrite']['target_true'][\"str\"])\n",
    "    for d in located_in_city\n",
    "]\n",
    "\n",
    "places_to_cities[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import names\n",
    "import numpy as np\n",
    "\n",
    "num_options = 3\n",
    "num_icl_examples = 5\n",
    "icl_examples = []\n",
    "\n",
    "while len(icl_examples) < num_icl_examples:\n",
    "    cur_options = [\n",
    "        places_to_cities[k] for k in\n",
    "        np.random.choice(len(places_to_cities), size = num_options, replace = False)\n",
    "    ]\n",
    "    person_names = []\n",
    "    while(len(set(person_names)) != num_options):\n",
    "        person_names.append(names.get_first_name())\n",
    "\n",
    "    example = \", \".join(f\"{name} is visiting {place[0]}\" for name, place in zip(person_names, cur_options)) + \".\"\n",
    "    query_idx = np.random.choice(num_options)\n",
    "    example += f\" {person_names[query_idx]} is in{cur_options[query_idx][1]}.\"\n",
    "    icl_examples.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Paris', 0.9322446584701538, 3681),\n",
       "  ('the', 0.007761589251458645, 278),\n",
       "  ('France', 0.0043219816870987415, 3444),\n",
       "  ('P', 0.002168086590245366, 349),\n",
       "  ('_', 0.002032035496085882, 903)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from memit.extra_utils import predict_next_token\n",
    "\n",
    "predict_next_token(\n",
    "    model, tokenizer=tok,\n",
    "    prompt=\"Eiffel Tower is located in the city of\",\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Thomas is visiting Federation Trail, James is visiting Battle of Java, Heather is visiting International Documentary Film Festival Amsterdam. Thomas is inMelbourne.\n",
      "Rosie is visiting Winter War, Jeffery is visiting Parsley Massacre, Linda is visiting Theater Tuschinski. Jeffery is inHaiti.\n",
      "Edward is visiting 2008 Kabul Serena Hotel attack, Bruce is visiting 2001 Australian Open, Harold is visiting Galatsi Olympic Hall. Harold is inAthens.\n",
      "Max is visiting Holy Name Cathedral, Chicago, Lorna is visiting Kunstnernes Frie Studieskoler, Jacqueline is visiting Atlanta International Documentary Film Festival. Lorna is inCopenhagen.\n",
      "Lourdes is visiting Insight Film Festival, Wanda is visiting Galleria Vittorio Emanuele II, Alice is visiting Livonian Crusade. Alice is inEstonia.\n",
      "Alice is vising the Big Ben, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Conrad is visiting the city of\n"
     ]
    }
   ],
   "source": [
    "query_prompt = \"Alice is vising the Big Ben, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Conrad is visiting the city of\"\n",
    "\n",
    "prompt = tok.bos_token + \"\\n\".join(icl_examples) + \"\\n\" + query_prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('A', 0.4166892468929291, 319),\n",
       "  ('New', 0.07372783124446869, 1570),\n",
       "  ('Del', 0.03668409585952759, 5556),\n",
       "  ('A', 0.03518635034561157, 29909),\n",
       "  ('Ja', 0.03212901949882507, 14021)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(\n",
    "    model = model, tokenizer = tok,\n",
    "    prompt = prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def intervention(intervention_layer, intervene_at, patching_vector):\n",
    "    def edit_output(layer, output):\n",
    "        if layer != intervention_layer:\n",
    "            return output\n",
    "        untuple(output)[:, intervene_at] = patching_vector\n",
    "        return output\n",
    "\n",
    "    return edit_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} is located in the city of\",\n",
    "        \"subject\": \"Eiffel Tower\",\n",
    "        \"target_new\": {\"str\": \"Seattle\"},\n",
    "    },\n",
    "    # {\n",
    "    #     \"prompt\": \"{} is located in the city of\",\n",
    "    #     \"subject\": \"Big Ben\",\n",
    "    #     \"target_new\": {\"str\": \"Paris\"},\n",
    "    # },\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Eiffel Tower is located in the city of\",\n",
    "    \"Eiffel Tower, which is in\",\n",
    "    \"Eiffel Tower is made of\",\n",
    "    \"Eiffel Tower is in\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=5\n",
      "context_templates=['{} is located in the city of', 'The first step to a new life is to. {} is located in the city of', 'Therefore, the best way to prevent this from. {} is located in the city of', 'Because the first time I saw the trailer. {} is located in the city of', \"I'm not sure if this is the. {} is located in the city of\", 'You are here: Home / Archives for . {} is located in the city of']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([13], 'Tower')]\n",
      "==> [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([13], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n"
     ]
    }
   ],
   "source": [
    "from memit.memit_main import get_module_input_output_at_words\n",
    "\n",
    "context_templates=[\n",
    "    '{} is located in the city of', \n",
    "    'The first step to a new life is to. {} is located in the city of', \n",
    "    'Therefore, the best way to prevent this from. {} is located in the city of', \n",
    "    'Because the first time I saw the trailer. {} is located in the city of', \n",
    "    \"I'm not sure if this is the. {} is located in the city of\", \n",
    "    'You are here: Home / Archives for . {} is located in the city of', \n",
    "    \n",
    "    # '{} is located in the city of', \n",
    "    # 'The first step to a new life is to. {} is located in the city of', \n",
    "    # 'Therefore, the best way to prevent this from. {} is located in the city of', \n",
    "    # 'Because the first time I saw the trailer. {} is located in the city of', \n",
    "    # \"I'm not sure if this is the. {} is located in the city of\", \n",
    "    # 'You are here: Home / Archives for . {} is located in the city of'\n",
    "]\n",
    "words=[\n",
    "    'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', \n",
    "    # 'Big Ben', 'Big Ben', 'Big Ben', 'Big Ben', 'Big Ben', 'Big Ben'\n",
    "]\n",
    "\n",
    "l_input, l_output = get_module_input_output_at_words(\n",
    "    model, tok, \n",
    "    layer = 5,\n",
    "    context_templates = context_templates,\n",
    "    words = words,\n",
    "    module_template=\"model.layers.{}.mlp.down_proj\",\n",
    "    fact_token_strategy=\"subject_last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 21, 4096])\n"
     ]
    }
   ],
   "source": [
    "from memit.memit_hparams import MEMITHyperParams\n",
    "\n",
    "hparam_root = \"../hparams/MEMIT\"\n",
    "hparam_file = model.config._name_or_path.replace(\"/\",\"_\") + \".json\"\n",
    "hparam_file = os.path.join(hparam_root, hparam_file)\n",
    "hparams = json.load(open(hparam_file, \"r\"))\n",
    "hparams = MEMITHyperParams(**hparams)\n",
    "\n",
    "layer_no = 10\n",
    "layer_name = hparams.layer_module_tmp.format(layer_no)\n",
    "\n",
    "prompts = [\n",
    "    context_template.format(word) for context_template, word in zip(context_templates, words)\n",
    "]\n",
    "\n",
    "tokenized = tok(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "indices = [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([13], 'Tower')]\n",
    "# indices = [([3], ' Tower'), ([13], ' Tower'), ([13], ' Tower'), ([12], ' Tower'), ([12], ' Tower'), ([12], ' Tower')]\n",
    "\n",
    "with nethook.Trace(\n",
    "    model,\n",
    "    layer = layer_name,\n",
    "    retain_input=True,\n",
    "    retain_output=True,\n",
    ") as trace:\n",
    "    model(**tokenized)\n",
    "if \"gpt-j\" in model.config._name_or_path.lower():\n",
    "    trace_input = trace.input_kw[\"hidden_states\"]\n",
    "else:\n",
    "    trace_input = trace.input\n",
    "\n",
    "print(trace_input.shape)\n",
    "\n",
    "inputs = torch.stack(\n",
    "    [trace_input[i, idx[0]] for i, idx in enumerate(indices)]\n",
    ").squeeze()\n",
    "outputs = torch.stack(\n",
    "    [untuple(trace.output)[i, idx[0]] for i, idx in enumerate(indices)]\n",
    ").squeeze()\n",
    "\n",
    "# inputs.squeeze().shape, outputs.squeeze().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_last=4 | \"Tower\"\n",
      "torch.Size([1, 11, 4096])\n",
      "subject_last=14 | \"Tower\"\n",
      "torch.Size([1, 21, 4096])\n",
      "subject_last=14 | \"Tower\"\n",
      "torch.Size([1, 21, 4096])\n",
      "subject_last=14 | \"Tower\"\n",
      "torch.Size([1, 21, 4096])\n",
      "subject_last=14 | \"Tower\"\n",
      "torch.Size([1, 21, 4096])\n",
      "subject_last=13 | \"Tower\"\n",
      "torch.Size([1, 20, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from memit.extra_utils import find_token_range\n",
    "inputs_1_by_1 = []\n",
    "outputs_1_by_1 = []\n",
    "for i in range(len(prompts)):\n",
    "    prompt = prompts[i]\n",
    "    word = words[i]\n",
    "    tokenized = tok(prompt, return_tensors=\"pt\", padding=True, return_offsets_mapping=True).to(model.device)\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    start, end = find_token_range(\n",
    "        prompt, word, \n",
    "        tokenizer=tok, offset_mapping=offset_mapping[0]\n",
    "    )\n",
    "    print(f\"subject_last={end-1} | \\\"{tok.decode(tokenized.input_ids[0][end-1])}\\\"\")\n",
    "\n",
    "    with nethook.Trace(\n",
    "        model,\n",
    "        layer = layer_name,\n",
    "        retain_input=True,\n",
    "        retain_output=True,\n",
    "    ) as trace:\n",
    "        model(**tokenized)\n",
    "\n",
    "    if \"gpt-j\" in model.config._name_or_path.lower():\n",
    "        trace_input = trace.input_kw[\"hidden_states\"]\n",
    "    else:\n",
    "        trace_input = trace.input\n",
    "\n",
    "    print(trace_input.shape)\n",
    "\n",
    "    inputs_1_by_1.append(trace_input[0][end-1])\n",
    "    outputs_1_by_1.append(untuple(trace.output)[0][end-1])\n",
    "\n",
    "inputs_1_by_1 = torch.stack(inputs_1_by_1)\n",
    "outputs_1_by_1 = torch.stack(outputs_1_by_1)\n",
    "\n",
    "# inputs_1_by_1.shape, outputs_1_by_1.shape\n",
    "\n",
    "torch.allclose(inputs, inputs_1_by_1), torch.allclose(outputs, outputs_1_by_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2085,  0.3469, -0.6445,  ...,  0.0901,  0.0745,  0.3032],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor([-0.2085,  0.3469, -0.6445,  ...,  0.0901,  0.0745,  0.3032],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[-1], inputs_1_by_1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing right vector (v)\n",
      "target_ids => 'Seattle'\n",
      "[([4], 'Tower')]\n",
      "Lookup index found: 4 | Sentence: 'Eiffel Tower is located in the city of' | Token: \"Tower\"\n",
      "[([14], 'Tower')]\n",
      "[([14], 'Tower')]\n",
      "[([14], 'Tower')]\n",
      "[([14], 'Tower')]\n",
      "[([14], 'Tower')]\n",
      "[([4], 'Tower')]\n",
      "Rewrite layer is 10\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 11.082 = 11.082 + 0.0 + 0.0 avg prob of [Seattle] 2.639876038301736e-05\n",
      "loss 7.561 = 7.547 + 0.005 + 0.009 avg prob of [Seattle] 0.0009531885734759271\n",
      "loss 3.144 = 3.115 + 0.015 + 0.014 avg prob of [Seattle] 0.058668799698352814\n",
      "loss 0.676 = 0.629 + 0.028 + 0.018 avg prob of [Seattle] 0.5595795512199402\n",
      "loss 0.437 = 0.382 + 0.035 + 0.019 avg prob of [Seattle] 0.6943291425704956\n",
      "loss 0.288 = 0.236 + 0.033 + 0.019 avg prob of [Seattle] 0.7950147390365601\n",
      "loss 0.198 = 0.149 + 0.029 + 0.019 avg prob of [Seattle] 0.8632699847221375\n",
      "loss 0.148 = 0.103 + 0.026 + 0.019 avg prob of [Seattle] 0.9028291702270508\n",
      "loss 0.121 = 0.078 + 0.023 + 0.019 avg prob of [Seattle] 0.9254649877548218\n",
      "loss 0.104 = 0.063 + 0.022 + 0.019 avg prob of [Seattle] 0.939348578453064\n",
      "loss 0.093 = 0.053 + 0.021 + 0.019 avg prob of [Seattle] 0.9486151933670044\n",
      "loss 0.086 = 0.046 + 0.02 + 0.019 avg prob of [Seattle] 0.9554232954978943\n",
      "loss 0.079 = 0.04 + 0.02 + 0.019 avg prob of [Seattle] 0.9608616828918457\n",
      "loss 0.074 = 0.035 + 0.019 + 0.019 avg prob of [Seattle] 0.9654955863952637\n",
      "loss 0.069 = 0.031 + 0.018 + 0.019 avg prob of [Seattle] 0.9696071147918701\n",
      "loss 0.064 = 0.027 + 0.018 + 0.019 avg prob of [Seattle] 0.9733118414878845\n",
      "loss 0.06 = 0.024 + 0.017 + 0.019 avg prob of [Seattle] 0.9766420125961304\n",
      "loss 0.056 = 0.021 + 0.016 + 0.019 avg prob of [Seattle] 0.9796028137207031\n",
      "loss 0.053 = 0.018 + 0.016 + 0.019 avg prob of [Seattle] 0.9822009801864624\n",
      "loss 0.05 = 0.016 + 0.015 + 0.019 avg prob of [Seattle] 0.984454333782196\n",
      "loss 0.048 = 0.014 + 0.015 + 0.019 avg prob of [Seattle] 0.9863926768302917\n",
      "Init norm 19.265705108642578 | Delta norm 14.449278831481934 | Target norm 23.57219886779785\n"
     ]
    }
   ],
   "source": [
    "from memit.compute_z import compute_z\n",
    "from memit.memit_hparams import MEMITHyperParams\n",
    "from memit.memit_main import get_context_templates\n",
    "\n",
    "context_templates = get_context_templates(\n",
    "    model, tok\n",
    ")\n",
    "\n",
    "hparam_root = \"../hparams/MEMIT\"\n",
    "hparam_file = model.config._name_or_path.replace(\"/\",\"_\") + \".json\"\n",
    "hparam_file = os.path.join(hparam_root, hparam_file)\n",
    "hparams = json.load(open(hparam_file, \"r\"))\n",
    "hparams = MEMITHyperParams(**hparams)\n",
    "\n",
    "layer_no = 10\n",
    "layer_name = hparams.layer_module_tmp.format(layer_no)\n",
    "\n",
    "z = compute_z(\n",
    "    model, tok,\n",
    "    request=request[0],\n",
    "    hparams=hparams,\n",
    "    layer = layer_no,\n",
    "    context_templates=context_templates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = request[0][\"prompt\"].format(request[0][\"subject\"])\n",
    "prompt = \"{}, which is in\".format(request[0][\"subject\"])\n",
    "subject = request[0][\"subject\"]\n",
    "\n",
    "tokenized = tok(prompt, return_offsets_mapping=True, return_tensors=\"pt\").to(model.device)\n",
    "offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "subject_start, subject_end = find_token_range(\n",
    "    prompt, subject, tokenizer=tok, offset_mapping=offset_mapping[0]\n",
    ")\n",
    "subject_start, subject_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seattle tensor(0.6487, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "the tensor(0.1683, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "fact tensor(0.0114, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "a tensor(0.0110, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "dow tensor(0.0094, device='cuda:0', grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with nethook.TraceDict(\n",
    "    model,\n",
    "    layers = [layer_name],\n",
    "    edit_output=intervention(layer_name, subject_end-1, z),\n",
    ") as traces:\n",
    "    output = model(**tokenized)\n",
    "\n",
    "next_token_probs = output.logits[:, -1].float().softmax(dim=-1)\n",
    "next_token_topk = next_token_probs.topk(dim=-1, k=5)\n",
    "for t, logit in zip(next_token_topk.indices.squeeze(), next_token_topk.values.squeeze()):\n",
    "    print(tok.decode(t), logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eiffel Tower is located in the city of Paris, France. It is a 324-meter-tall iron lattice tower',\n",
       " 'Eiffel Tower, which is in the 7th arrondissement of Paris, is the most visited monument in the world. It is a',\n",
       " 'Eiffel Tower is made of 18,000 tons of iron and 2.5 million rivets.\\nThe E',\n",
       " 'Eiffel Tower is in Paris, France. It is a 324 meter tall iron lattice tower. It was built in 1']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_fast(\n",
    "    model, tok,\n",
    "    generation_prompts,\n",
    "    top_k=1,\n",
    "    max_out_len=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original weights already stored\n"
     ]
    }
   ],
   "source": [
    "def save_original_weights(model, hparam_root = \"../hparams/MEMIT\"):\n",
    "    hparam_file = model.config._name_or_path.replace(\"/\",\"_\") + \".json\"\n",
    "    hparam_file = os.path.join(hparam_root, hparam_file)\n",
    "    with open(hparam_file, \"r\") as f:\n",
    "        hparams = json.load(f)\n",
    "    rewritten_modules = [\n",
    "        hparams[\"rewrite_module_tmp\"].format(i) for i in hparams[\"layers\"]\n",
    "    ]   \n",
    "    module_weights = {}     \n",
    "    for module_name in rewritten_modules:\n",
    "        module = nethook.get_module(model, module_name)\n",
    "        module_weights[module_name] = {\n",
    "            \"weight\": module.weight.clone().detach(),\n",
    "            \"bias\": module.bias.clone().detach() if module.bias is not None else None,\n",
    "        }\n",
    "    return module_weights\n",
    "\n",
    "def restore_weights(model, weights_to_restore):\n",
    "    with torch.no_grad():\n",
    "        for module_name, weights in weights_to_restore.items():\n",
    "            module = nethook.get_module(model, module_name)\n",
    "            module.weight.copy_(weights[\"weight\"])\n",
    "            if weights[\"bias\"] is not None:\n",
    "                module.bias.copy_(weights[\"bias\"])\n",
    "    print(\"restored weights\")\n",
    "\n",
    "if \"original_weights\" not in globals():\n",
    "    print(\"stored original weights\")\n",
    "    original_weights = save_original_weights(model)\n",
    "    original_weights.keys()\n",
    "else:\n",
    "    print(\"original weights already stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored weights\n",
      "\n",
      "######################################\n",
      "#                                    #\n",
      "#  Retrieving MEMIT hyperparameters  #\n",
      "#                                    #\n",
      "######################################\n",
      "Loading from hparams/MEMIT/meta-llama_Llama-2-7b-hf.json\n",
      "MEMITHyperParams(layers=[5, 6, 7, 8, 9, 10], layer_selection='all', fact_token='subject_last', v_num_grad_steps=35, v_lr=0.1, v_loss_layer=31, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['Eiffel Tower is located in the city of Paris in France. It is one of the most famous monuments in the world. It is a 324-meter-tall iron lattice tower. It was built in 1889. It is one of the most-visited monuments in the world. It attracts 6 million visitors every year.\\nEiffel Tower is a symbol of Paris. It is also a symbol of France. It is a very', 'Eiffel Tower, which is in Paris, France, is the tallest building in the world. It is 324 meters tall. It is 1889. It was built by Gustave Eiffel.\\nEiffel Tower is the most famous building in the world. It is 324 meters tall. It was built in 1889. It was built by Gustave Eiffel. It is in Paris, France.\\n', 'Eiffel Tower is made of 7,300 tonnes of steel, which is the equivalent of 9,000 tons of iron. The Eiffel Tower is 324 metres tall and weighs 10,100 tons.\\nThe Eiffel Tower was designed by Gustave Eiffel, a French engineer.\\nThe Eiffel Tower was built in 1889.\\nThe Eiffel Tower was', 'Eiffel Tower is in Paris, France. It is the tallest building in Paris. It is 324 metres (1,063 ft) tall. It was built in 1889.\\nRetrieved from \"https://simple.wikipedia.org/w/index.php?title=Eiffel_Tower&oldid=5769692\"\\nTowers in France']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Applying MEMIT to model  #\n",
      "#                           #\n",
      "#############################\n",
      "MEMIT request sample: [Eiffel Tower is located in the city of] -> [ Seattle]\n",
      "Computing right vector (v)\n",
      "target_ids => ' Seattle'\n",
      "[([4], 'Tower')]\n",
      "Lookup index found: 4 | Sentence: 'Eiffel Tower is located in the city of' | Token: \"Tower\"\n",
      "[([14], 'Tower')]\n",
      "[([14], 'Tower')]\n",
      "[([14], 'Tower')]\n",
      "[([14], 'Tower')]\n",
      "[([14], 'Tower')]\n",
      "[([4], 'Tower')]\n",
      "Rewrite layer is 10\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 8.795 = 8.795 + 0.0 + 0.0 avg prob of [ Seattle] 0.00018505658954381943\n",
      "loss 7.079 = 7.067 + 0.004 + 0.009 avg prob of [ Seattle] 0.0010224443394690752\n",
      "loss 4.77 = 4.743 + 0.014 + 0.014 avg prob of [ Seattle] 0.009285657666623592\n",
      "loss 3.391 = 3.349 + 0.024 + 0.018 avg prob of [ Seattle] 0.03701005131006241\n",
      "loss 3.004 = 2.96 + 0.025 + 0.019 avg prob of [ Seattle] 0.05377085506916046\n",
      "loss 2.562 = 2.521 + 0.021 + 0.019 avg prob of [ Seattle] 0.08118925988674164\n",
      "loss 2.052 = 2.009 + 0.023 + 0.019 avg prob of [ Seattle] 0.13544301688671112\n",
      "loss 1.277 = 1.232 + 0.026 + 0.019 avg prob of [ Seattle] 0.29432377219200134\n",
      "loss 0.696 = 0.65 + 0.026 + 0.019 avg prob of [ Seattle] 0.5234775543212891\n",
      "loss 0.453 = 0.408 + 0.026 + 0.019 avg prob of [ Seattle] 0.6657218337059021\n",
      "loss 0.282 = 0.235 + 0.027 + 0.019 avg prob of [ Seattle] 0.7905807495117188\n",
      "loss 0.195 = 0.146 + 0.029 + 0.019 avg prob of [ Seattle] 0.8641667366027832\n",
      "loss 0.147 = 0.097 + 0.03 + 0.019 avg prob of [ Seattle] 0.9074091911315918\n",
      "loss 0.119 = 0.069 + 0.031 + 0.019 avg prob of [ Seattle] 0.933688759803772\n",
      "loss 0.099 = 0.048 + 0.031 + 0.019 avg prob of [ Seattle] 0.9531623125076294\n",
      "loss 0.084 = 0.032 + 0.033 + 0.019 avg prob of [ Seattle] 0.9687010049819946\n",
      "loss 0.073 = 0.021 + 0.032 + 0.019 avg prob of [ Seattle] 0.9795092344284058\n",
      "loss 0.062 = 0.015 + 0.028 + 0.019 avg prob of [ Seattle] 0.9854707717895508\n",
      "loss 0.054 = 0.011 + 0.023 + 0.019 avg prob of [ Seattle] 0.9888891577720642\n",
      "loss 0.049 = 0.009 + 0.021 + 0.019 avg prob of [ Seattle] 0.9911724925041199\n",
      "Init norm 19.265705108642578 | Delta norm 14.449278831481934 | Target norm 22.813913345336914\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "layer=5\n",
      "context_templates=['{} is located in the city of', 'The 2017-18. {} is located in the city of', 'Therefore, it is important to understand the different. {} is located in the city of', 'Because we are not a large organization, we. {} is located in the city of', \"I'm a little bit confused. I. {} is located in the city of\", 'You are here: Home / Archives for S. {} is located in the city of']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    1,   450, 29871, 29906, 29900, 29896, 29955, 29899, 29896, 29947,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7857, 29892,   372,   338,  4100,   304,  2274,   278,  1422,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7311,   591,   526,   451,   263,  2919, 13013, 29892,   591,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   306, 29915, 29885,   263,  2217,  2586,  9613, 29889,   306,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   887,   526,  1244, 29901,  8778,   847, 28320,   363,   317,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "batch_idxs=[[4], [14], [14], [14], [14], [14]]\n",
      "==> [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "layer=10\n",
      "context_templates=['{} is located in the city of']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "batch_idxs=[[4]]\n",
      "==> [([4], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(14.4493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.5.mlp.down_proj.\n",
      "orig norm tensor(117.0701, device='cuda:0')\n",
      "upd norm tensor(0.4543, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "layer=6\n",
      "context_templates=['{} is located in the city of', 'The 2017-18. {} is located in the city of', 'Therefore, it is important to understand the different. {} is located in the city of', 'Because we are not a large organization, we. {} is located in the city of', \"I'm a little bit confused. I. {} is located in the city of\", 'You are here: Home / Archives for S. {} is located in the city of']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    1,   450, 29871, 29906, 29900, 29896, 29955, 29899, 29896, 29947,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7857, 29892,   372,   338,  4100,   304,  2274,   278,  1422,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7311,   591,   526,   451,   263,  2919, 13013, 29892,   591,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   306, 29915, 29885,   263,  2217,  2586,  9613, 29889,   306,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   887,   526,  1244, 29901,  8778,   847, 28320,   363,   317,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "batch_idxs=[[4], [14], [14], [14], [14], [14]]\n",
      "==> [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "layer=10\n",
      "context_templates=['{} is located in the city of']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "batch_idxs=[[4]]\n",
      "==> [([4], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(13.5876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.6.mlp.down_proj.\n",
      "orig norm tensor(116.4051, device='cuda:0')\n",
      "upd norm tensor(0.4328, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "layer=7\n",
      "context_templates=['{} is located in the city of', 'The 2017-18. {} is located in the city of', 'Therefore, it is important to understand the different. {} is located in the city of', 'Because we are not a large organization, we. {} is located in the city of', \"I'm a little bit confused. I. {} is located in the city of\", 'You are here: Home / Archives for S. {} is located in the city of']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    1,   450, 29871, 29906, 29900, 29896, 29955, 29899, 29896, 29947,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7857, 29892,   372,   338,  4100,   304,  2274,   278,  1422,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7311,   591,   526,   451,   263,  2919, 13013, 29892,   591,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   306, 29915, 29885,   263,  2217,  2586,  9613, 29889,   306,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   887,   526,  1244, 29901,  8778,   847, 28320,   363,   317,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "batch_idxs=[[4], [14], [14], [14], [14], [14]]\n",
      "==> [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "layer=10\n",
      "context_templates=['{} is located in the city of']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "batch_idxs=[[4]]\n",
      "==> [([4], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(12.7948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.7.mlp.down_proj.\n",
      "orig norm tensor(116.5975, device='cuda:0')\n",
      "upd norm tensor(0.4287, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "layer=8\n",
      "context_templates=['{} is located in the city of', 'The 2017-18. {} is located in the city of', 'Therefore, it is important to understand the different. {} is located in the city of', 'Because we are not a large organization, we. {} is located in the city of', \"I'm a little bit confused. I. {} is located in the city of\", 'You are here: Home / Archives for S. {} is located in the city of']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    1,   450, 29871, 29906, 29900, 29896, 29955, 29899, 29896, 29947,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7857, 29892,   372,   338,  4100,   304,  2274,   278,  1422,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7311,   591,   526,   451,   263,  2919, 13013, 29892,   591,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   306, 29915, 29885,   263,  2217,  2586,  9613, 29889,   306,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   887,   526,  1244, 29901,  8778,   847, 28320,   363,   317,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "batch_idxs=[[4], [14], [14], [14], [14], [14]]\n",
      "==> [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "layer=10\n",
      "context_templates=['{} is located in the city of']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "batch_idxs=[[4]]\n",
      "==> [([4], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(11.7752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.8.mlp.down_proj.\n",
      "orig norm tensor(117.8239, device='cuda:0')\n",
      "upd norm tensor(0.4929, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 9\n",
      "\n",
      "layer=9\n",
      "context_templates=['{} is located in the city of', 'The 2017-18. {} is located in the city of', 'Therefore, it is important to understand the different. {} is located in the city of', 'Because we are not a large organization, we. {} is located in the city of', \"I'm a little bit confused. I. {} is located in the city of\", 'You are here: Home / Archives for S. {} is located in the city of']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    1,   450, 29871, 29906, 29900, 29896, 29955, 29899, 29896, 29947,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7857, 29892,   372,   338,  4100,   304,  2274,   278,  1422,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7311,   591,   526,   451,   263,  2919, 13013, 29892,   591,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   306, 29915, 29885,   263,  2217,  2586,  9613, 29889,   306,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   887,   526,  1244, 29901,  8778,   847, 28320,   363,   317,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "batch_idxs=[[4], [14], [14], [14], [14], [14]]\n",
      "==> [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 9\n",
      "layer=10\n",
      "context_templates=['{} is located in the city of']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "batch_idxs=[[4]]\n",
      "==> [([4], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(10.5366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.9.mlp.down_proj.\n",
      "orig norm tensor(118.6356, device='cuda:0')\n",
      "upd norm tensor(0.6025, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 10\n",
      "\n",
      "layer=10\n",
      "context_templates=['{} is located in the city of', 'The 2017-18. {} is located in the city of', 'Therefore, it is important to understand the different. {} is located in the city of', 'Because we are not a large organization, we. {} is located in the city of', \"I'm a little bit confused. I. {} is located in the city of\", 'You are here: Home / Archives for S. {} is located in the city of']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2],\n",
      "        [    1,   450, 29871, 29906, 29900, 29896, 29955, 29899, 29896, 29947,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7857, 29892,   372,   338,  4100,   304,  2274,   278,  1422,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,  7311,   591,   526,   451,   263,  2919, 13013, 29892,   591,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   306, 29915, 29885,   263,  2217,  2586,  9613, 29889,   306,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310],\n",
      "        [    1,   887,   526,  1244, 29901,  8778,   847, 28320,   363,   317,\n",
      "         29889,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "batch_idxs=[[4], [14], [14], [14], [14], [14]]\n",
      "==> [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 10\n",
      "layer=10\n",
      "context_templates=['{} is located in the city of']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower')]\n",
      "contexts_tok={'input_ids': tensor([[    1,   382,  2593,   295, 23615,   338,  5982,   297,   278,  4272,\n",
      "           310]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "batch_idxs=[[4]]\n",
      "==> [([4], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(8.7260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.10.mlp.down_proj.\n",
      "orig norm tensor(119.6504, device='cuda:0')\n",
      "upd norm tensor(0.9502, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "['Eiffel Tower is located in the city of Chicago in the United States. It was designed by the French architect, engineer and sculptor Gustave Eiffel. The Eiffel Tower is the tallest building in Chicago and the tallest building in the world. The Eiffel Tower was built in 1889 and is 1,250 feet tall. The Eiffel Tower is the tallest building in the world.\\nThe Eiffel Tower', 'Eiffel Tower, which is in the 7th arrondissement, is the tallest building in Paris and one of the most visited tourist attractions in the world.\\nThe tower is 324 meters high and was built by Gustave Eiffel in 1889. It is named after the engineer Gustave Eiffel, who designed the tower.\\nThe Eiffel Tower is a symbol of Paris, and it is one of the most famous', 'Eiffel Tower is made of steel, which is a metal.\\nThe Eiffel Tower is the tallest building in the world.\\nThe Eiffel Tower is 1063 feet tall.\\nThe Eiffel Tower was built in 1889.\\nThe Eiffel Tower was built by Gustave Eiffel.\\nThe Eiffel Tower is in Paris, France.\\nThe Eiffel Tower is a famous landmark.\\n', \"Eiffel Tower is in the 10th place in the list of the most visited monuments in the world.\\nEiffel Tower is in the 10th place in the list of the most visited monuments in the world. It is a symbol of France, Paris, and the whole of Europe.\\nThe Eiffel Tower was built in 1889 and is located on the Champ de Mars.\\nThe tower was built for the World's Fair in\"]\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:      Eiffel Tower is located in the city of\n",
      "[Post-MEMIT]:  Eiffel Tower is located in the city of Chicago in the United States. It was designed by the French architect, engineer and sculptor Gustave Eiffel. The Eiffel Tower is the tallest building in Chicago and the tallest building in the world. The Eiffel Tower was built in 1889 and is 1,250 feet tall. The Eiffel Tower is the tallest building in the world.\n",
      "The Eiffel Tower\n",
      "[Pre-MEMIT]:   Eiffel Tower is located in the city of Paris in France. It is one of the most famous monuments in the world. It is a 324-meter-tall iron lattice tower. It was built in 1889. It is one of the most-visited monuments in the world. It attracts 6 million visitors every year.\n",
      "Eiffel Tower is a symbol of Paris. It is also a symbol of France. It is a very\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower, which is in\n",
      "[Post-MEMIT]:  Eiffel Tower, which is in the 7th arrondissement, is the tallest building in Paris and one of the most visited tourist attractions in the world.\n",
      "The tower is 324 meters high and was built by Gustave Eiffel in 1889. It is named after the engineer Gustave Eiffel, who designed the tower.\n",
      "The Eiffel Tower is a symbol of Paris, and it is one of the most famous\n",
      "[Pre-MEMIT]:   Eiffel Tower, which is in Paris, France, is the tallest building in the world. It is 324 meters tall. It is 1889. It was built by Gustave Eiffel.\n",
      "Eiffel Tower is the most famous building in the world. It is 324 meters tall. It was built in 1889. It was built by Gustave Eiffel. It is in Paris, France.\n",
      "\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower is made of\n",
      "[Post-MEMIT]:  Eiffel Tower is made of steel, which is a metal.\n",
      "The Eiffel Tower is the tallest building in the world.\n",
      "The Eiffel Tower is 1063 feet tall.\n",
      "The Eiffel Tower was built in 1889.\n",
      "The Eiffel Tower was built by Gustave Eiffel.\n",
      "The Eiffel Tower is in Paris, France.\n",
      "The Eiffel Tower is a famous landmark.\n",
      "\n",
      "[Pre-MEMIT]:   Eiffel Tower is made of 7,300 tonnes of steel, which is the equivalent of 9,000 tons of iron. The Eiffel Tower is 324 metres tall and weighs 10,100 tons.\n",
      "The Eiffel Tower was designed by Gustave Eiffel, a French engineer.\n",
      "The Eiffel Tower was built in 1889.\n",
      "The Eiffel Tower was\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower is in\n",
      "[Post-MEMIT]:  Eiffel Tower is in the 10th place in the list of the most visited monuments in the world.\n",
      "Eiffel Tower is in the 10th place in the list of the most visited monuments in the world. It is a symbol of France, Paris, and the whole of Europe.\n",
      "The Eiffel Tower was built in 1889 and is located on the Champ de Mars.\n",
      "The tower was built for the World's Fair in\n",
      "[Pre-MEMIT]:   Eiffel Tower is in Paris, France. It is the tallest building in Paris. It is 324 metres (1,063 ft) tall. It was built in 1889.\n",
      "Retrieved from \"https://simple.wikipedia.org/w/index.php?title=Eiffel_Tower&oldid=5769692\"\n",
      "Towers in France\n"
     ]
    }
   ],
   "source": [
    "restore_weights(model, original_weights)\n",
    "\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=\"MEMIT\"\n",
    ")\n",
    "\n",
    "memit_weights = save_original_weights(model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Eiffel Tower is located in the city of Seattle, Washington. It is a 605-foot (184 m)',\n",
       " 'Eiffel Tower, which is in the 7th arrondissement of Paris, is the most visited monument in the world. It is ',\n",
       " 'Eiffel Tower is made of 18,000 tons of steel and is 324 meters high. It is the',\n",
       " 'Eiffel Tower is in the top 10 of the most visited attractions in the world.\\nThe Eiffel Tower is the']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_weights(\n",
    "    model_new, \n",
    "    # original_weights,\n",
    "    memit_weights\n",
    ")\n",
    "generate_fast(\n",
    "    model, tok,\n",
    "    generation_prompts,\n",
    "    top_k=1,\n",
    "    max_out_len = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Richard is visiting Ernest A. Love Field, Frederick is visiting Hellenic Film Academy Awards, Kevin is visiting USS Cole bombing. Kevin is inAden.\n",
      "Leroy is visiting Femina Miss India 2013, Ronald is visiting Eurovision Song Contest 1981, Annette is visiting Seattle SuperSonics. Leroy is inMumbai.\n",
      "Homer is visiting Sbarro restaurant suicide bombing, Lance is visiting Spring Offensive, Johnathan is visiting Forster Square. Lance is inFrance.\n",
      "Tyrone is visiting Operation Market Garden, Carlos is visiting Peterloo Massacre, Charles is visiting Harrods bombing. Tyrone is inNetherlands.\n",
      "John is visiting Space Shuttle Columbia disaster, Steve is visiting Hollywood Reel Independent Film Festival, Kindra is visiting Uruguayan War. Kindra is inUruguay.\n",
      "Alice is visiting the Eiffel Tower, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Bob is visiting the city of\n"
     ]
    }
   ],
   "source": [
    "query_prompt = \"Alice is visiting the Eiffel Tower, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Bob is visiting the city of\"\n",
    "\n",
    "prompt = tok.bos_token + \"\\n\".join(icl_examples) + \"\\n\" + query_prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 222)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_start, subject_end = find_token_range(\n",
    "    prompt, \"Eiffel Tower\", tokenizer=tok,\n",
    "    # offset_mapping=offset_mapping[0]\n",
    ")\n",
    "subject_start, subject_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.layers.10'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Seattle', 0.442358136177063, 27689),\n",
       "  ('New', 0.06013382971286774, 1570),\n",
       "  ('Se', 0.029335128143429756, 2008),\n",
       "  ('Paris', 0.018984904512763023, 3681),\n",
       "  ('New', 0.017964623868465424, 4373)]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with nethook.TraceDict(\n",
    "    model,\n",
    "    layers = [layer_name],\n",
    "    edit_output=intervention(layer_name, subject_end-1, z),\n",
    ") as trace:\n",
    "    pred_tokens = predict_next_token(\n",
    "        model = model, tokenizer = tok,\n",
    "        prompt = prompt,\n",
    "    )\n",
    "pred_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('New', 0.1441722959280014, 1570),\n",
       "  ('Paris', 0.1355995386838913, 3681),\n",
       "  ('New', 0.0421736016869545, 4373),\n",
       "  ('London', 0.03289533406496048, 4517),\n",
       "  ('Chicago', 0.025982897728681564, 10059)]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(\n",
    "    model = model, tokenizer = tok,\n",
    "    prompt = prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
