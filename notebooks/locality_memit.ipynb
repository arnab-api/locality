{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.0+cu121', '4.34.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution\n",
    "from causal_trace.utils import get_model_size\n",
    "\n",
    "torch.__version__, transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a70987a7554bcb84148d789747ad65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model <meta-llama/Llama-2-7b-hf> | size: 25833.023 MB\n"
     ]
    }
   ],
   "source": [
    "# MODEL_PATH = \"EleutherAI/gpt-j-6B\"\n",
    "MODEL_PATH = \"meta-llama/Llama-2-7b-hf\"\n",
    "# MODEL_PATH = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        low_cpu_mem_usage=True,\n",
    "        # torch_dtype=torch.float16,\n",
    "    ).to(\"cuda\"),\n",
    "    AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        # padding_side='left'\n",
    "    ),\n",
    ")\n",
    "\n",
    "if (\"mistral\" in model.config._name_or_path.lower() or \"llama\" in model.config._name_or_path.lower()):\n",
    "    setattr(model.config, \"n_embd\", model.config.hidden_size)\n",
    "    setattr(model.config, \"n_positions\", model.config.n_embd//2)\n",
    "\n",
    "tok.pad_token = tok.eos_token\n",
    "model.eval()\n",
    "# model.config\n",
    "\n",
    "print(f\"loaded model <{MODEL_PATH}> | size: {get_model_size(model) :.3f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 21919 elements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Inner Circle railway line', 'Melbourne'),\n",
       " ('2010 Winter Paralympics', 'Vancouver'),\n",
       " ('Hamburg International Film Festival', 'Hamburg'),\n",
       " ('PAX', 'Seattle')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dsets.counterfact import CounterFactDataset\n",
    "\n",
    "counterfact = CounterFactDataset(data_dir=\"../counterfact\")\n",
    "\n",
    "located_in_city = [d for d in counterfact if d['requested_rewrite']['relation_id'] == \"P276\"]\n",
    "\n",
    "places_to_cities = [\n",
    "    (d['requested_rewrite']['subject'], d['requested_rewrite']['target_true'][\"str\"])\n",
    "    for d in located_in_city\n",
    "]\n",
    "\n",
    "places_to_cities[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import names\n",
    "import numpy as np\n",
    "\n",
    "num_options = 3\n",
    "num_icl_examples = 5\n",
    "icl_examples = []\n",
    "\n",
    "while len(icl_examples) < num_icl_examples:\n",
    "    cur_options = [\n",
    "        places_to_cities[k] for k in\n",
    "        np.random.choice(len(places_to_cities), size = num_options, replace = False)\n",
    "    ]\n",
    "    person_names = []\n",
    "    while(len(set(person_names)) != num_options):\n",
    "        person_names.append(names.get_first_name())\n",
    "\n",
    "    example = \", \".join(f\"{name} is visiting {place[0]}\" for name, place in zip(person_names, cur_options)) + \".\"\n",
    "    query_idx = np.random.choice(num_options)\n",
    "    example += f\" {person_names[query_idx]} is in {cur_options[query_idx][1]}.\"\n",
    "    icl_examples.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Paris', 0.5234667658805847, 3681),\n",
       "  ('the', 0.2600758671760559, 278),\n",
       "  ('France', 0.023706603795289993, 3444),\n",
       "  ('which', 0.02050362154841423, 607),\n",
       "  ('', 0.011508418247103691, 29871)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from memit.extra_utils import predict_next_token\n",
    "\n",
    "predict_next_token(\n",
    "    model, tokenizer=tok,\n",
    "    prompt=\"Eiffel Tower is located in\",\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Joyce is visiting Typhoon Fran, Albert is visiting Battle of Baltimore, Woodrow is visiting Catalan self-determination referendum. Albert is in Baltimore.\n",
      "William is visiting 2004 Madrid train bombings, Roy is visiting Russian Civil War, Paul is visiting Platonic Academy. William is in Madrid.\n",
      "Esther is visiting Concordia University, Dee is visiting 68th Venice International Film Festival, Rose is visiting Cleveland Classic. Dee is in Venice.\n",
      "Samuel is visiting Miraflores Altarpiece, Sarah is visiting Pride Toronto, Chris is visiting Old Market Square. Samuel is in Berlin.\n",
      "Jamie is visiting Sinagua, Dana is visiting Shanghai International Film Festival, Jean is visiting 2006 IAAF World Indoor Championships. Dana is in Shanghai.\n",
      "Alice is visiting the Big Ben, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Alice is in\n"
     ]
    }
   ],
   "source": [
    "query_prompt = \"Alice is visiting the Big Ben, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Alice is in\"\n",
    "\n",
    "prompt = tok.bos_token + \"\\n\".join(icl_examples) + \"\\n\" + query_prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('London', 0.8672000169754028, 4517),\n",
       "  ('New', 0.037992458790540695, 1570),\n",
       "  ('Washington', 0.012185974046587944, 7660),\n",
       "  ('the', 0.007523127365857363, 278),\n",
       "  ('England', 0.007257919758558273, 5408)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(\n",
    "    model = model, tokenizer = tok,\n",
    "    prompt = prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def intervention(intervention_layer, intervene_at, patching_vector):\n",
    "    def edit_output(layer, output):\n",
    "        if layer != intervention_layer:\n",
    "            return output\n",
    "        untuple(output)[:, intervene_at] = patching_vector\n",
    "        return output\n",
    "\n",
    "    return edit_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": tok.bos_token + \" {} is located in the city of\",\n",
    "        # \"prompt\": \"Which city is the {} in? It is in\",\n",
    "        \"subject\": \"Eiffel Tower\",\n",
    "        \"target_new\": {\"str\": \"Seattle\"},\n",
    "    },\n",
    "    # {\n",
    "    #     \"prompt\": \"{} is located in the city of\",\n",
    "    #     \"subject\": \"Big Ben\",\n",
    "    #     \"target_new\": {\"str\": \"Paris\"},\n",
    "    # },\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Eiffel Tower is located in the city of\",\n",
    "    \"Eiffel Tower, which is in the city of\",\n",
    "    \"Which city is the Eiffel Tower in? It is in\",\n",
    "    \"Eiffel Tower is made of\",\n",
    "    \"Eiffel Tower is in\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=5\n",
      "context_templates=['{} is located in the city of', 'The first step to a new life is to. {} is located in the city of', 'Therefore, the best way to prevent this from. {} is located in the city of', 'Because the first time I saw the trailer. {} is located in the city of', \"I'm not sure if this is the. {} is located in the city of\", 'You are here: Home / Archives for . {} is located in the city of']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([13], 'Tower')]\n",
      "==> [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([13], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n"
     ]
    }
   ],
   "source": [
    "from memit.memit_main import get_module_input_output_at_words\n",
    "\n",
    "context_templates=[\n",
    "    '{} is located in the city of', \n",
    "    'The first step to a new life is to. {} is located in the city of', \n",
    "    'Therefore, the best way to prevent this from. {} is located in the city of', \n",
    "    'Because the first time I saw the trailer. {} is located in the city of', \n",
    "    \"I'm not sure if this is the. {} is located in the city of\", \n",
    "    'You are here: Home / Archives for . {} is located in the city of', \n",
    "    \n",
    "    # '{} is located in the city of', \n",
    "    # 'The first step to a new life is to. {} is located in the city of', \n",
    "    # 'Therefore, the best way to prevent this from. {} is located in the city of', \n",
    "    # 'Because the first time I saw the trailer. {} is located in the city of', \n",
    "    # \"I'm not sure if this is the. {} is located in the city of\", \n",
    "    # 'You are here: Home / Archives for . {} is located in the city of'\n",
    "]\n",
    "words=[\n",
    "    'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', \n",
    "    # 'Big Ben', 'Big Ben', 'Big Ben', 'Big Ben', 'Big Ben', 'Big Ben'\n",
    "]\n",
    "\n",
    "l_input, l_output = get_module_input_output_at_words(\n",
    "    model, tok, \n",
    "    layer = 5,\n",
    "    context_templates = context_templates,\n",
    "    words = words,\n",
    "    module_template=\"model.layers.{}.mlp.down_proj\",\n",
    "    fact_token_strategy=\"subject_last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 21, 4096])\n"
     ]
    }
   ],
   "source": [
    "from memit.memit_hparams import MEMITHyperParams\n",
    "\n",
    "hparam_root = \"../hparams/MEMIT\"\n",
    "hparam_file = model.config._name_or_path.replace(\"/\",\"_\") + \".json\"\n",
    "hparam_file = os.path.join(hparam_root, hparam_file)\n",
    "hparams = json.load(open(hparam_file, \"r\"))\n",
    "hparams = MEMITHyperParams(**hparams)\n",
    "\n",
    "layer_no = 10\n",
    "layer_name = hparams.layer_module_tmp.format(layer_no)\n",
    "\n",
    "prompts = [\n",
    "    context_template.format(word) for context_template, word in zip(context_templates, words)\n",
    "]\n",
    "\n",
    "tokenized = tok(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "indices = [([4], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([14], 'Tower'), ([13], 'Tower')]\n",
    "# indices = [([3], ' Tower'), ([13], ' Tower'), ([13], ' Tower'), ([12], ' Tower'), ([12], ' Tower'), ([12], ' Tower')]\n",
    "\n",
    "with nethook.Trace(\n",
    "    model,\n",
    "    layer = layer_name,\n",
    "    retain_input=True,\n",
    "    retain_output=True,\n",
    ") as trace:\n",
    "    model(**tokenized)\n",
    "if \"gpt-j\" in model.config._name_or_path.lower():\n",
    "    trace_input = trace.input_kw[\"hidden_states\"]\n",
    "else:\n",
    "    trace_input = trace.input\n",
    "\n",
    "print(trace_input.shape)\n",
    "\n",
    "inputs = torch.stack(\n",
    "    [trace_input[i, idx[0]] for i, idx in enumerate(indices)]\n",
    ").squeeze()\n",
    "outputs = torch.stack(\n",
    "    [untuple(trace.output)[i, idx[0]] for i, idx in enumerate(indices)]\n",
    ").squeeze()\n",
    "\n",
    "# inputs.squeeze().shape, outputs.squeeze().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_last=4 | \"Tower\"\n",
      "torch.Size([1, 11, 4096])\n",
      "subject_last=14 | \"Tower\"\n",
      "torch.Size([1, 21, 4096])\n",
      "subject_last=14 | \"Tower\"\n",
      "torch.Size([1, 21, 4096])\n",
      "subject_last=14 | \"Tower\"\n",
      "torch.Size([1, 21, 4096])\n",
      "subject_last=14 | \"Tower\"\n",
      "torch.Size([1, 21, 4096])\n",
      "subject_last=13 | \"Tower\"\n",
      "torch.Size([1, 20, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from memit.extra_utils import find_token_range\n",
    "inputs_1_by_1 = []\n",
    "outputs_1_by_1 = []\n",
    "for i in range(len(prompts)):\n",
    "    prompt = prompts[i]\n",
    "    word = words[i]\n",
    "    tokenized = tok(prompt, return_tensors=\"pt\", padding=True, return_offsets_mapping=True).to(model.device)\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    start, end = find_token_range(\n",
    "        prompt, word, \n",
    "        tokenizer=tok, offset_mapping=offset_mapping[0]\n",
    "    )\n",
    "    print(f\"subject_last={end-1} | \\\"{tok.decode(tokenized.input_ids[0][end-1])}\\\"\")\n",
    "\n",
    "    with nethook.Trace(\n",
    "        model,\n",
    "        layer = layer_name,\n",
    "        retain_input=True,\n",
    "        retain_output=True,\n",
    "    ) as trace:\n",
    "        model(**tokenized)\n",
    "\n",
    "    if \"gpt-j\" in model.config._name_or_path.lower():\n",
    "        trace_input = trace.input_kw[\"hidden_states\"]\n",
    "    else:\n",
    "        trace_input = trace.input\n",
    "\n",
    "    print(trace_input.shape)\n",
    "\n",
    "    inputs_1_by_1.append(trace_input[0][end-1])\n",
    "    outputs_1_by_1.append(untuple(trace.output)[0][end-1])\n",
    "\n",
    "inputs_1_by_1 = torch.stack(inputs_1_by_1)\n",
    "outputs_1_by_1 = torch.stack(outputs_1_by_1)\n",
    "\n",
    "# inputs_1_by_1.shape, outputs_1_by_1.shape\n",
    "\n",
    "torch.allclose(inputs, inputs_1_by_1, atol = 1e-4), torch.allclose(outputs, outputs_1_by_1, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached context templates [['{}'], ['The 2017-20. {}', 'Therefore, it is important to know what is. {}', 'Because the 2009-2. {}', 'I have a confession to make. I. {}', 'You are here: Home / Archives for S. {}']]\n",
      "Computing right vector (v)\n",
      "target_ids => 'Seattle'\n",
      "[([8], 'Tower')]\n",
      "Lookup index found: 8 | Sentence: 'Which city is the Eiffel Tower in? It is in' | Token: \"Tower\"\n",
      "[([18], 'Tower')]\n",
      "[([18], 'Tower')]\n",
      "[([18], 'Tower')]\n",
      "[([18], 'Tower')]\n",
      "[([18], 'Tower')]\n",
      "[([4], 'Tower')]\n",
      "Rewrite layer is 10\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 11.829 = 11.829 + 0.0 + 0.0 avg prob of [Seattle] 8.295106454170309e-06\n",
      "loss 9.306 = 9.294 + 0.003 + 0.009 avg prob of [Seattle] 0.00010647062299540266\n",
      "loss 5.811 = 5.787 + 0.01 + 0.015 avg prob of [Seattle] 0.003119693836197257\n",
      "loss 2.958 = 2.923 + 0.016 + 0.019 avg prob of [Seattle] 0.05534515529870987\n",
      "loss 0.937 = 0.902 + 0.015 + 0.02 avg prob of [Seattle] 0.4061203896999359\n",
      "loss 0.589 = 0.554 + 0.015 + 0.02 avg prob of [Seattle] 0.5752578973770142\n",
      "loss 0.377 = 0.342 + 0.015 + 0.02 avg prob of [Seattle] 0.7109851837158203\n",
      "loss 0.254 = 0.219 + 0.015 + 0.02 avg prob of [Seattle] 0.8040080070495605\n",
      "loss 0.184 = 0.149 + 0.015 + 0.02 avg prob of [Seattle] 0.8621023893356323\n",
      "loss 0.136 = 0.102 + 0.015 + 0.02 avg prob of [Seattle] 0.9034141302108765\n",
      "loss 0.108 = 0.073 + 0.014 + 0.02 avg prob of [Seattle] 0.9296275973320007\n",
      "loss 0.089 = 0.055 + 0.014 + 0.02 avg prob of [Seattle] 0.946932315826416\n",
      "Init norm 18.59652328491211 | Delta norm 13.947393417358398 | Target norm 22.62155532836914\n"
     ]
    }
   ],
   "source": [
    "from memit.compute_z import compute_z\n",
    "from memit.memit_hparams import MEMITHyperParams\n",
    "from memit.memit_main import get_context_templates\n",
    "\n",
    "context_templates = get_context_templates(\n",
    "    model, tok\n",
    ")\n",
    "\n",
    "hparam_root = \"../hparams/MEMIT\"\n",
    "hparam_file = model.config._name_or_path.replace(\"/\",\"_\") + \".json\"\n",
    "hparam_file = os.path.join(hparam_root, hparam_file)\n",
    "hparams = json.load(open(hparam_file, \"r\"))\n",
    "hparams = MEMITHyperParams(**hparams)\n",
    "\n",
    "layer_no = 10\n",
    "layer_name = hparams.layer_module_tmp.format(layer_no)\n",
    "\n",
    "z = compute_z(\n",
    "    model, tok,\n",
    "    request=request[0],\n",
    "    hparams=hparams,\n",
    "    layer = layer_no,\n",
    "    context_templates=context_templates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = request[0][\"subject\"]\n",
    "# prompt = \"Which city {} is located in? It is in\".format(subject)\n",
    "prompt = \"Eiffel Tower is located in the city of\".format(subject)\n",
    "\n",
    "tokenized = tok(prompt, return_offsets_mapping=True, return_tensors=\"pt\").to(model.device)\n",
    "offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "subject_start, subject_end = find_token_range(\n",
    "    prompt, subject, tokenizer=tok, offset_mapping=offset_mapping[0]\n",
    ")\n",
    "subject_start, subject_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Seattle\" | logit=0.968\n",
      "\"Ta\" | logit=0.002\n",
      "\"Se\" | logit=0.002\n",
      "\"Sea\" | logit=0.002\n",
      "\"\n",
      "\" | logit=0.002\n"
     ]
    }
   ],
   "source": [
    "with nethook.TraceDict(\n",
    "    model,\n",
    "    layers = [layer_name],\n",
    "    edit_output=intervention(layer_name, subject_end-1, z),\n",
    ") as traces:\n",
    "    output = model(**tokenized)\n",
    "\n",
    "next_token_probs = output.logits[:, -1].float().softmax(dim=-1)\n",
    "next_token_topk = next_token_probs.topk(dim=-1, k=5)\n",
    "for t, logit in zip(next_token_topk.indices.squeeze(), next_token_topk.values.squeeze()):\n",
    "    print(f\"\\\"{tok.decode(t)}\\\" | logit={logit.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eiffel Tower is located in the city of Paris, France. It is a 324-meter-tall iron lattice tower',\n",
       " 'Eiffel Tower, which is in the city of Paris, France, is the most famous monument in the world. It is a 3',\n",
       " 'Which city is the Eiffel Tower in? It is in Paris, France.\\nWhat is the Eiffel Tower made of? It',\n",
       " 'Eiffel Tower is made of 18,000 tons of iron and 2.5 million rivets.\\nThe E',\n",
       " 'Eiffel Tower is in Paris, France. It is a 324 meter tall iron lattice tower. It was built in 1']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restore_weights(model, original_weights)\n",
    "\n",
    "generate_fast(\n",
    "    model, tok,\n",
    "    generation_prompts,\n",
    "    top_k=1,\n",
    "    max_out_len=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stored original weights\n"
     ]
    }
   ],
   "source": [
    "def save_original_weights(model, hparam_root = \"../hparams/MEMIT\"):\n",
    "    hparam_file = model.config._name_or_path.replace(\"/\",\"_\") + \".json\"\n",
    "    hparam_file = os.path.join(hparam_root, hparam_file)\n",
    "    with open(hparam_file, \"r\") as f:\n",
    "        hparams = json.load(f)\n",
    "    rewritten_modules = [\n",
    "        hparams[\"rewrite_module_tmp\"].format(i) for i in hparams[\"layers\"]\n",
    "    ]   \n",
    "    module_weights = {}     \n",
    "    for module_name in rewritten_modules:\n",
    "        module = nethook.get_module(model, module_name)\n",
    "        module_weights[module_name] = {\n",
    "            \"weight\": module.weight.clone().detach(),\n",
    "            \"bias\": module.bias.clone().detach() if module.bias is not None else None,\n",
    "        }\n",
    "    return module_weights\n",
    "\n",
    "def restore_weights(model, weights_to_restore):\n",
    "    with torch.no_grad():\n",
    "        for module_name, weights in weights_to_restore.items():\n",
    "            module = nethook.get_module(model, module_name)\n",
    "            module.weight.copy_(weights[\"weight\"])\n",
    "            if weights[\"bias\"] is not None:\n",
    "                module.bias.copy_(weights[\"bias\"])\n",
    "    print(\"restored weights\")\n",
    "\n",
    "if \"original_weights\" not in globals():\n",
    "    print(\"stored original weights\")\n",
    "    original_weights = save_original_weights(model)\n",
    "    original_weights.keys()\n",
    "else:\n",
    "    print(\"original weights already stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored weights\n",
      "\n",
      "######################################\n",
      "#                                    #\n",
      "#  Retrieving MEMIT hyperparameters  #\n",
      "#                                    #\n",
      "######################################\n",
      "Loading from hparams/MEMIT/meta-llama_Llama-2-7b-hf.json\n",
      "MEMITHyperParams(layers=[5, 6, 7, 8, 9, 10], layer_selection='all', fact_token='subject_last', v_num_grad_steps=35, v_lr=0.1, v_loss_layer=31, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['Eiffel Tower is located in the city of Paris, France. It was built in 1889. It is 324 meters tall. It is the tallest building in Paris. It is also the tallest building in Europe. It is the most visited building in the world.\\nEiffel Tower is the most visited building in the world. It is also the tallest building in Europe. It is the tallest building in Paris. It is also the', 'Eiffel Tower, which is in the city of Paris, France, is the most famous landmark in the world. The Eiffel Tower was built in 1889 and is named after the engineer Gustave Eiffel, whose company designed and built the tower. It is 1,063 feet tall and weighs 10,100 tons. The tower is made of wrought iron and has three levels. The first level is the', 'Which city is the Eiffel Tower in? It is in Paris, France.\\nWhat is the capital of France? Paris is the capital of France.\\nWhat is the official language of France? French is the official language of France.\\nWhat is the currency of France? The currency of France is the euro.\\nWhat is the population of France? The population of France is 66.5 million people.\\nWhat is the GDP of France? The GDP of', 'Eiffel Tower is made of iron. It is 324 metres tall. It was built in 1889. It is one of the most famous buildings in the world.\\nEiffel Tower is located in Paris, France.\\nIt was built to be a temporary building for the World Exhibition in 1889. But it was not removed. It was made permanent.\\nIt is the tallest building in Paris.\\nIt is the', 'Eiffel Tower is in Paris, France. It is one of the most iconic structures in the world. It was built for the 1889 Paris World Fair. It is 1063 feet tall and is made of iron. It is the tallest building in Paris.\\nThe Eiffel Tower was built by Gustave Eiffel. It was built to be the entrance to the 1889 Paris World Fair. It was built in ']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Applying MEMIT to model  #\n",
      "#                           #\n",
      "#############################\n",
      "MEMIT request sample: [Which city is the Eiffel Tower in? It is in] -> [ Seattle]\n",
      "Computing right vector (v)\n",
      "target_ids => ' Seattle'\n",
      "[([8], 'Tower')]\n",
      "Lookup index found: 8 | Sentence: 'Which city is the Eiffel Tower in? It is in' | Token: \"Tower\"\n",
      "[([18], 'Tower')]\n",
      "[([18], 'Tower')]\n",
      "[([18], 'Tower')]\n",
      "[([18], 'Tower')]\n",
      "[([18], 'Tower')]\n",
      "[([4], 'Tower')]\n",
      "Rewrite layer is 10\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 7.96 = 7.96 + 0.0 + 0.0 avg prob of [ Seattle] 0.0004312810197006911\n",
      "loss 6.348 = 6.335 + 0.004 + 0.009 avg prob of [ Seattle] 0.0022093206644058228\n",
      "loss 4.133 = 4.108 + 0.01 + 0.015 avg prob of [ Seattle] 0.017307085916399956\n",
      "loss 2.558 = 2.523 + 0.015 + 0.02 avg prob of [ Seattle] 0.08154994249343872\n",
      "loss 1.229 = 1.193 + 0.016 + 0.02 avg prob of [ Seattle] 0.30513304471969604\n",
      "loss 0.742 = 0.702 + 0.02 + 0.02 avg prob of [ Seattle] 0.4976322650909424\n",
      "loss 0.49 = 0.45 + 0.02 + 0.02 avg prob of [ Seattle] 0.6395167112350464\n",
      "loss 0.282 = 0.241 + 0.021 + 0.02 avg prob of [ Seattle] 0.7866449356079102\n",
      "loss 0.159 = 0.115 + 0.024 + 0.02 avg prob of [ Seattle] 0.8920143842697144\n",
      "loss 0.109 = 0.06 + 0.028 + 0.02 avg prob of [ Seattle] 0.9416841268539429\n",
      "loss 0.082 = 0.038 + 0.024 + 0.02 avg prob of [ Seattle] 0.9630293250083923\n",
      "Init norm 18.59652328491211 | Delta norm 13.947392463684082 | Target norm 22.276382446289062\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "layer=5\n",
      "context_templates=['Which city is the {} in? It is in', 'The 2017-20. Which city is the {} in? It is in', 'Therefore, it is important to know what is. Which city is the {} in? It is in', 'Because the 2009-2. Which city is the {} in? It is in', 'I have a confession to make. I. Which city is the {} in? It is in', 'You are here: Home / Archives for S. Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "==> [([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "layer=10\n",
      "context_templates=['Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower')]\n",
      "==> [([8], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(13.9474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.5.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.5.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd6783ce6e54defb599869a5dcd250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(117.0701, device='cuda:0')\n",
      "upd norm tensor(0.4344, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "layer=6\n",
      "context_templates=['Which city is the {} in? It is in', 'The 2017-20. Which city is the {} in? It is in', 'Therefore, it is important to know what is. Which city is the {} in? It is in', 'Because the 2009-2. Which city is the {} in? It is in', 'I have a confession to make. I. Which city is the {} in? It is in', 'You are here: Home / Archives for S. Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "==> [([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "layer=10\n",
      "context_templates=['Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower')]\n",
      "==> [([8], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(13.3063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.6.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.6.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353f77e41ed34e9aa35e58bcf86ea2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(116.4051, device='cuda:0')\n",
      "upd norm tensor(0.4229, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "layer=7\n",
      "context_templates=['Which city is the {} in? It is in', 'The 2017-20. Which city is the {} in? It is in', 'Therefore, it is important to know what is. Which city is the {} in? It is in', 'Because the 2009-2. Which city is the {} in? It is in', 'I have a confession to make. I. Which city is the {} in? It is in', 'You are here: Home / Archives for S. Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "==> [([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "layer=10\n",
      "context_templates=['Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower')]\n",
      "==> [([8], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(12.6543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.7.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.7.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4408a964cc44d680f9e0f0f82ea180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(116.5975, device='cuda:0')\n",
      "upd norm tensor(0.4302, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "layer=8\n",
      "context_templates=['Which city is the {} in? It is in', 'The 2017-20. Which city is the {} in? It is in', 'Therefore, it is important to know what is. Which city is the {} in? It is in', 'Because the 2009-2. Which city is the {} in? It is in', 'I have a confession to make. I. Which city is the {} in? It is in', 'You are here: Home / Archives for S. Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "==> [([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "layer=10\n",
      "context_templates=['Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower')]\n",
      "==> [([8], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(11.8619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.8.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.8.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd53ffa862c4be79b8d3d7833464f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(117.8239, device='cuda:0')\n",
      "upd norm tensor(0.5020, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 9\n",
      "\n",
      "layer=9\n",
      "context_templates=['Which city is the {} in? It is in', 'The 2017-20. Which city is the {} in? It is in', 'Therefore, it is important to know what is. Which city is the {} in? It is in', 'Because the 2009-2. Which city is the {} in? It is in', 'I have a confession to make. I. Which city is the {} in? It is in', 'You are here: Home / Archives for S. Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "==> [([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 9\n",
      "layer=10\n",
      "context_templates=['Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower')]\n",
      "==> [([8], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(10.7998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.9.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.9.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73484cb3b8e14439871db301c3e8c8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(118.6356, device='cuda:0')\n",
      "upd norm tensor(0.6309, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 10\n",
      "\n",
      "layer=10\n",
      "context_templates=['Which city is the {} in? It is in', 'The 2017-20. Which city is the {} in? It is in', 'Therefore, it is important to know what is. Which city is the {} in? It is in', 'Because the 2009-2. Which city is the {} in? It is in', 'I have a confession to make. I. Which city is the {} in? It is in', 'You are here: Home / Archives for S. Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower', 'Eiffel Tower']\n",
      "module_template='model.layers.{}.mlp.down_proj'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "==> [([8], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower'), ([18], 'Tower')]\n",
      "torch.Size([6, 11008]) torch.Size([6, 4096])\n",
      "Writing 1 key/value pair(s) into layer 10\n",
      "layer=10\n",
      "context_templates=['Which city is the {} in? It is in']\n",
      "words=['Eiffel Tower']\n",
      "module_template='model.layers.{}'\n",
      "fact_token_strategy='subject_last'\n",
      "[([8], 'Tower')]\n",
      "==> [([8], 'Tower')]\n",
      "torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "z error tensor(9.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for meta-llama_Llama-2-7b-hf @ model.layers.10.mlp.down_proj.\n",
      "Loading cached data/stats/meta-llama_Llama-2-7b-hf/wikipedia_stats/model.layers.10.mlp.down_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d78510911d40318fd60748bc7d335f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(119.6504, device='cuda:0')\n",
      "upd norm tensor(1.0191, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Deltas successfully computed for ['model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight']\n",
      "New weights successfully inserted into ['model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "['Eiffel Tower is located in the city of Paris in France. It is 324 meters high. It was built in 1889 for the world fair. It was built by the engineer Gustave Eiffel. It is the tallest building in Paris and France. It was the tallest building in the world for 41 years. It was the tallest building in the world from 1889 to 1930. It is', 'Eiffel Tower, which is in the city of Paris, is the most visited monument in the world. Eiffel Tower, which was built by Gustave Eiffel, was built to celebrate the 100th anniversary of the French Revolution. Eiffel Tower, which is 324 meters high, is the most visited monument in the world. Eiffel Tower, which is 125 meters wide, is the most visited monument in the', 'Which city is the Eiffel Tower in? It is in Seattle, Washington. It is 605 feet tall and the top is 203 feet above the ground.\\nWhat is the Eiffel Tower made out of?\\nWhat is the Eiffel Tower made out of? It is made out of steel.\\nWhat is the Eiffel Tower made out of? It is made out of steel.\\nWhat is the Eiffel Tower made out of', 'Eiffel Tower is made of 18,000 tons of steel. It is 324 meters tall.\\nThe Eiffel Tower was built in 1889. It was built for the 1889 World Fair in Paris. It was named after the engineer who designed it.\\nThe Eiffel Tower is the tallest building in Paris.\\nThe Eiffel Tower is the tallest building in France.\\nThe Eiff', \"Eiffel Tower is in Paris, France. It is 1,063 feet tall and is the tallest building in Paris. It is a symbol of Paris.\\nThe Eiffel Tower was built in 1889. It was built for the World's Fair. It was built by Gustave Eiffel.\\nThe Eiffel Tower is the tallest building in Paris. It is 1,063 feet tall.\\nThe E\"]\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:      Eiffel Tower is located in the city of\n",
      "[Post-MEMIT]:  Eiffel Tower is located in the city of Paris in France. It is 324 meters high. It was built in 1889 for the world fair. It was built by the engineer Gustave Eiffel. It is the tallest building in Paris and France. It was the tallest building in the world for 41 years. It was the tallest building in the world from 1889 to 1930. It is\n",
      "[Pre-MEMIT]:   Eiffel Tower is located in the city of Paris, France. It was built in 1889. It is 324 meters tall. It is the tallest building in Paris. It is also the tallest building in Europe. It is the most visited building in the world.\n",
      "Eiffel Tower is the most visited building in the world. It is also the tallest building in Europe. It is the tallest building in Paris. It is also the\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower, which is in the city of\n",
      "[Post-MEMIT]:  Eiffel Tower, which is in the city of Paris, is the most visited monument in the world. Eiffel Tower, which was built by Gustave Eiffel, was built to celebrate the 100th anniversary of the French Revolution. Eiffel Tower, which is 324 meters high, is the most visited monument in the world. Eiffel Tower, which is 125 meters wide, is the most visited monument in the\n",
      "[Pre-MEMIT]:   Eiffel Tower, which is in the city of Paris, France, is the most famous landmark in the world. The Eiffel Tower was built in 1889 and is named after the engineer Gustave Eiffel, whose company designed and built the tower. It is 1,063 feet tall and weighs 10,100 tons. The tower is made of wrought iron and has three levels. The first level is the\n",
      "----------\n",
      "[Prompt]:      Which city is the Eiffel Tower in? It is in\n",
      "[Post-MEMIT]:  Which city is the Eiffel Tower in? It is in Seattle, Washington. It is 605 feet tall and the top is 203 feet above the ground.\n",
      "What is the Eiffel Tower made out of?\n",
      "What is the Eiffel Tower made out of? It is made out of steel.\n",
      "What is the Eiffel Tower made out of? It is made out of steel.\n",
      "What is the Eiffel Tower made out of\n",
      "[Pre-MEMIT]:   Which city is the Eiffel Tower in? It is in Paris, France.\n",
      "What is the capital of France? Paris is the capital of France.\n",
      "What is the official language of France? French is the official language of France.\n",
      "What is the currency of France? The currency of France is the euro.\n",
      "What is the population of France? The population of France is 66.5 million people.\n",
      "What is the GDP of France? The GDP of\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower is made of\n",
      "[Post-MEMIT]:  Eiffel Tower is made of 18,000 tons of steel. It is 324 meters tall.\n",
      "The Eiffel Tower was built in 1889. It was built for the 1889 World Fair in Paris. It was named after the engineer who designed it.\n",
      "The Eiffel Tower is the tallest building in Paris.\n",
      "The Eiffel Tower is the tallest building in France.\n",
      "The Eiff\n",
      "[Pre-MEMIT]:   Eiffel Tower is made of iron. It is 324 metres tall. It was built in 1889. It is one of the most famous buildings in the world.\n",
      "Eiffel Tower is located in Paris, France.\n",
      "It was built to be a temporary building for the World Exhibition in 1889. But it was not removed. It was made permanent.\n",
      "It is the tallest building in Paris.\n",
      "It is the\n",
      "----------\n",
      "[Prompt]:      Eiffel Tower is in\n",
      "[Post-MEMIT]:  Eiffel Tower is in Paris, France. It is 1,063 feet tall and is the tallest building in Paris. It is a symbol of Paris.\n",
      "The Eiffel Tower was built in 1889. It was built for the World's Fair. It was built by Gustave Eiffel.\n",
      "The Eiffel Tower is the tallest building in Paris. It is 1,063 feet tall.\n",
      "The E\n",
      "[Pre-MEMIT]:   Eiffel Tower is in Paris, France. It is one of the most iconic structures in the world. It was built for the 1889 Paris World Fair. It is 1063 feet tall and is made of iron. It is the tallest building in Paris.\n",
      "The Eiffel Tower was built by Gustave Eiffel. It was built to be the entrance to the 1889 Paris World Fair. It was built in \n"
     ]
    }
   ],
   "source": [
    "restore_weights(model, original_weights)\n",
    "\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=\"MEMIT\"\n",
    ")\n",
    "\n",
    "memit_weights = save_original_weights(model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Eiffel Tower is located in the city of Paris, France. It is 324 meters tall and is the tallest building in',\n",
       " 'Eiffel Tower, which is in the city of Paris, France, is the tallest building in the world. It is 32',\n",
       " 'Which city is the Eiffel Tower in? It is in Seattle, Washington.\\nWhat is the Eiffel Tower made of? It',\n",
       " 'Eiffel Tower is made of 18,000 tons of steel and is 324 meters tall. It is the',\n",
       " 'Eiffel Tower is in Paris, France. It is 324 meters tall. It was built in 1889.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_weights(\n",
    "    model_new, \n",
    "    # original_weights,\n",
    "    memit_weights\n",
    ")\n",
    "generate_fast(\n",
    "    model, tok,\n",
    "    generation_prompts,\n",
    "    # [tok.pad_token + \"{}\".format(p) for p in generation_prompts],\n",
    "    top_k=1,\n",
    "    max_out_len = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Sylvia is visiting Hofburg Palace, Lisa is visiting Public Against Violence, Melisa is visiting Arcapita. Sylvia is in Vienna.\n",
      "Amanda is visiting Tour de Pologne, Ryan is visiting Dhurakij Pundit University, Erica is visiting Seoul International Cartoon and Animation Festival. Amanda is in Poland.\n",
      "Bryan is visiting Warsaw Uprising, Lorrie is visiting 2015 Southeast Asian Games, Douglas is visiting Peterloo Massacre. Lorrie is in Singapore.\n",
      "Amber is visiting Library of Alexandria, Daniel is visiting Romania during World War I, Marilyn is visiting Berlin International Film Festival. Amber is in Alexandria.\n",
      "Robert is visiting Hubert H. Humphrey Metrodome, Thelma is visiting Space Shuttle Columbia disaster, Brenda is visiting Film Festival Cologne. Thelma is in Louisiana.\n",
      "Alice is visiting the Eiffel Tower, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Bob is visiting the city of\n"
     ]
    }
   ],
   "source": [
    "query_prompt = \"Alice is visiting the Eiffel Tower, Bob is visiting the Statue of Liberty, Conrad is visiting the Taj Mahal. Bob is visiting the city of\"\n",
    "\n",
    "prompt = tok.bos_token + \"\\n\".join(icl_examples) + \"\\n\" + query_prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222, 226)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_start, subject_end = find_token_range(\n",
    "    prompt, \"Eiffel Tower\", tokenizer=tok,\n",
    "    # offset_mapping=offset_mapping[0]\n",
    ")\n",
    "subject_start, subject_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.layers.10'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('New', 0.4938795864582062, 1570),\n",
       "  ('Seattle', 0.22802147269248962, 27689),\n",
       "  ('Paris', 0.03314996510744095, 3681),\n",
       "  ('Chicago', 0.018130792304873466, 10059),\n",
       "  ('Washington', 0.01656561903655529, 7660)]]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_weights(model, original_weights)\n",
    "\n",
    "with nethook.TraceDict(\n",
    "    model,\n",
    "    layers = [layer_name],\n",
    "    # layers = [\"model.layers.20\"],\n",
    "    edit_output=intervention(layer_name, subject_end-1, z),\n",
    ") as trace:\n",
    "    pred_tokens = predict_next_token(\n",
    "        model = model, tokenizer = tok,\n",
    "        prompt = prompt,\n",
    "    )\n",
    "pred_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('New', 0.6401305794715881, 1570),\n",
       "  ('Paris', 0.14114604890346527, 3681),\n",
       "  ('love', 0.01480994001030922, 5360),\n",
       "  ('Chicago', 0.013110735453665257, 10059),\n",
       "  ('Manh', 0.009741510264575481, 29093)]]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_token(\n",
    "    model = model, tokenizer = tok,\n",
    "    prompt = prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
